<?xml version="1.0" ?>
<PsychoPy2experiment encoding="utf-8" version="2021.2.2">
  <Settings>
    <Param name="Audio latency priority" updates="None" val="use prefs" valType="str"/>
    <Param name="Audio lib" updates="None" val="ptb" valType="str"/>
    <Param name="Completed URL" updates="None" val="" valType="str"/>
    <Param name="Data file delimiter" updates="None" val="auto" valType="str"/>
    <Param name="Data filename" updates="None" val="u'data/%s_%s_%s' % (expInfo['participant'], expName, expInfo['date'])" valType="code"/>
    <Param name="Enable Escape" updates="None" val="True" valType="bool"/>
    <Param name="Experiment info" updates="None" val="{'participant': '', 'session': '001'}" valType="code"/>
    <Param name="Force stereo" updates="None" val="False" valType="bool"/>
    <Param name="Full-screen window" updates="None" val="True" valType="bool"/>
    <Param name="HTML path" updates="None" val="" valType="str"/>
    <Param name="Incomplete URL" updates="None" val="" valType="str"/>
    <Param name="Monitor" updates="None" val="spectrum_monitor" valType="str"/>
    <Param name="Resources" updates="None" val="[]" valType="fileList"/>
    <Param name="Save csv file" updates="None" val="False" valType="bool"/>
    <Param name="Save excel file" updates="None" val="False" valType="bool"/>
    <Param name="Save hdf5 file" updates="None" val="False" valType="bool"/>
    <Param name="Save log file" updates="None" val="True" valType="bool"/>
    <Param name="Save psydat file" updates="None" val="True" valType="bool"/>
    <Param name="Save wide csv file" updates="None" val="True" valType="bool"/>
    <Param name="Screen" updates="None" val="2" valType="num"/>
    <Param name="Show info dlg" updates="None" val="True" valType="bool"/>
    <Param name="Show mouse" updates="None" val="False" valType="bool"/>
    <Param name="Units" updates="None" val="deg" valType="str"/>
    <Param name="Use version" updates="None" val="" valType="str"/>
    <Param name="Window size (pixels)" updates="None" val="[1920, 1080]" valType="code"/>
    <Param name="blendMode" updates="None" val="avg" valType="str"/>
    <Param name="color" updates="None" val="$[1,1,1]" valType="str"/>
    <Param name="colorSpace" updates="None" val="rgb" valType="str"/>
    <Param name="elAddress" updates="None" val="100.1.1.1" valType="str"/>
    <Param name="elDataFiltering" updates="None" val="FILTER_LEVEL_2" valType="str"/>
    <Param name="elLiveFiltering" updates="None" val="FILTER_LEVEL_OFF" valType="str"/>
    <Param name="elModel" updates="None" val="EYELINK 1000 DESKTOP" valType="str"/>
    <Param name="elPupilAlgorithm" updates="None" val="ELLIPSE_FIT" valType="str"/>
    <Param name="elPupilMeasure" updates="None" val="PUPIL_AREA" valType="str"/>
    <Param name="elSampleRate" updates="None" val="1000" valType="num"/>
    <Param name="elSimMode" updates="None" val="False" valType="bool"/>
    <Param name="elTrackEyes" updates="None" val="RIGHT_EYE" valType="str"/>
    <Param name="elTrackingMode" updates="None" val="PUPIL_CR_TRACKING" valType="str"/>
    <Param name="expName" updates="None" val="infant_audiovisual_eyetrack" valType="str"/>
    <Param name="exportHTML" updates="None" val="on Sync" valType="str"/>
    <Param name="eyetracker" updates="None" val="None" valType="str"/>
    <Param name="gpAddress" updates="None" val="127.0.0.1" valType="str"/>
    <Param name="gpPort" updates="None" val="4242" valType="num"/>
    <Param name="logging level" updates="None" val="debug" valType="code"/>
    <Param name="mgBlink" updates="None" val="('MIDDLE_BUTTON',)" valType="list"/>
    <Param name="mgMove" updates="None" val="CONTINUOUS" valType="str"/>
    <Param name="mgSaccade" updates="None" val="0.5" valType="num"/>
    <Param name="tbLicenseFile" updates="None" val="" valType="str"/>
    <Param name="tbModel" updates="None" val="" valType="str"/>
    <Param name="tbSampleRate" updates="None" val="60" valType="num"/>
    <Param name="tbSerialNo" updates="None" val="" valType="str"/>
  </Settings>
  <Routines>
    <Routine name="setup">
      <CodeComponent name="code_setup">
        <Param name="Before Experiment" updates="constant" val="### BEGIN SET CONSTANTS ###&amp;#10;# unless otherwise specified,&amp;#10;# stimuli sizes/coordinates are specified&amp;#10;# in degrees (deg), and times are specified&amp;#10;# in seconds&amp;#10;&amp;#10;# notes about 'Kleberg 2019' refer to&amp;#10;# &quot;How Infants’ Arousal Influences Their Visual Search&quot;&amp;#10;# - Kleberg, del Bianco, Falck-Ytter 2019&amp;#10;&amp;#10;# variable for activating (True) or disabling (False)&amp;#10;# debug mode. make sure that this is set to &amp;#10;# **False**, or the variable and associated debugging&amp;#10;# code is entirely removed, before running the experiment&amp;#10;# with real participants.&amp;#10;# note that for the time being, **debug mode will crash**&amp;#10;# after 4 trials unless you also reduce the trial_loop to just&amp;#10;# 4 iterations. the reason is that less images are loaded, to&amp;#10;# decrease startup time.&amp;#10;DEBUG_ON = False&amp;#10;&amp;#10;# eyetracker update frequency (unfortunately this has to be updated&amp;#10;# both here and in YAML file), eg 600 if eyetracker captures&amp;#10;# gaze 600 times/second (600Hz).&amp;#10;EYETRACKER_HZ = 600&amp;#10;&amp;#10;# load screen image width/height (this is specified&amp;#10;# separately from other images, since it doesn't&amp;#10;# share specifications with anything)&amp;#10;LOAD_IMG_WIDTH = 15.65&amp;#10;LOAD_IMG_HEIGHT = 12&amp;#10;&amp;#10;&amp;#10;# visual stimuli distance from centre of screen,&amp;#10;# in x/y direction&amp;#10;IMG_Y_OFFSET = 6&amp;#10;IMG_X_OFFSET = 12&amp;#10;&amp;#10;# 'in-trial' visual stimuli max width/heights&amp;#10;# (stimuli are scaled, maintaining proportions,&amp;#10;# to being as large as possible without surpassing&amp;#10;# the maximum width/height)&amp;#10;IMG_MAX_WIDTH = 9&amp;#10;IMG_MAX_HEIGHT = 9&amp;#10;# attention grabber max width/heights&amp;#10;ATT_MAX_WIDTH = 5&amp;#10;ATT_MAX_HEIGHT = 5&amp;#10;&amp;#10;# each visual stimulus also has an associated area of interest&amp;#10;# (AOI; see trial code components for details on implementations&amp;#10;# of this), with its own (same as or larger than the stimulus) &amp;#10;# width/height.&amp;#10;# trial AOI&amp;#10;TRIAL_AOI_WIDTH = 10&amp;#10;TRIAL_AOI_HEIGHT = 10&amp;#10;# attention grabber AOI&amp;#10;ATT_AOI_WIDTH = 10&amp;#10;ATT_AOI_HEIGHT = 10&amp;#10;&amp;#10;# trial audio low/high volume levels. note that these describe&amp;#10;# *PsychoPy* sound levels, which are directly proportional to the&amp;#10;# physical sound/air pressure. this is different from decibels -&amp;#10;# look for info online if you need to understand the relationship.&amp;#10;TRIAL_AUDIO_LOW = 0.7&amp;#10;TRIAL_AUDIO_HIGH = 1&amp;#10;&amp;#10;# path to csv file which specifies image file paths,&amp;#10;# original image widths/heights (in pixels), type&amp;#10;# of image (social/geometric shape/man-made/natural/attention grabber),&amp;#10;# and sex (male/female)&amp;#10;IMG_SPEC_PATH = (&amp;#10;    'stimuli_specifications/visual_stimuli_specifications.csv'&amp;#10;)&amp;#10;# if debug mode is on, use less images to decrease load time&amp;#10;if DEBUG_ON:&amp;#10;    IMG_SPEC_PATH = (&amp;#10;        'stimuli_specifications/visual_stimuli_specifications_DEBUG.csv'&amp;#10;    )&amp;#10;&amp;#10;# path to csv file which specifies sound file paths,&amp;#10;# sound types (social/nonsocial) and sex (male/female/&amp;#10;# nonsocial)&amp;#10;AUDIO_SPEC_PATH = (&amp;#10;    'stimuli_specifications/auditory_stimuli_specifications.csv'&amp;#10;)&amp;#10;# path to csv file which specifies attention grabber&amp;#10;# image file paths and original image &amp;#10;# widths/heights (in pixels)&amp;#10;ATT_IMG_SPEC_PATH = (&amp;#10;    'stimuli_specifications/'&amp;#10;    'attentiongrabber_images_specifications.csv'&amp;#10;)&amp;#10;# path to csv file which specifies &amp;#10;# attention grabbing sound file paths&amp;#10;ATT_AUDIO_SPEC_PATH = (&amp;#10;    'stimuli_specifications/'&amp;#10;    'attentiongrabber_audio_specifications.csv'&amp;#10;)&amp;#10;&amp;#10;# attention grabber earliest/latest time of&amp;#10;# onset (counting from trial start time) &amp;#10;# -- onset directly at trial start, based on &amp;#10;# internal instruction from Falck-Ytter and Da Silva-- &amp;#10;ATT_GRAB_EARLIEST = 0&amp;#10;ATT_GRAB_LATEST = 0&amp;#10;&amp;#10;# attention grabber __maximum__ duration (ie if eyetracker is malfunctioning&amp;#10;# and/or the participant is not at all looking at the attention grabber,&amp;#10;# force experiment to continue after this duration)&amp;#10;ATT_GRAB_MAX_DUR = 5&amp;#10;&amp;#10;# sound min/max delay after gaze is directed&amp;#10;# at attention grabber&amp;#10;# -- onset directly after gaze capture, based on &amp;#10;# internal instruction from Falck-Ytter and Da Silva-- &amp;#10;AUDITORY_MIN_DELAY = 0&amp;#10;AUDITORY_MAX_DELAY = 0&amp;#10;&amp;#10;# min/max delay from sound to&amp;#10;# visual stimuli onset &amp;#10;# -- based on Kleberg 2019 --&amp;#10;VISUAL_MIN_DELAY = 0.08&amp;#10;VISUAL_MAX_DELAY = 0.4&amp;#10;# visual stimuli &amp;#10;# min/max presentation duration&amp;#10;# -- based on Kleberg 2019 --&amp;#10;VISUAL_MIN_DUR = 3&amp;#10;VISUAL_MAX_DUR = 3&amp;#10;&amp;#10;# min/max duration of 'blank' screen after&amp;#10;# visual stimuli have finished (and before&amp;#10;# trial end)&amp;#10;# -- based on internal instruction from Falck-Ytter &amp;#10;# and Da Silva--&amp;#10;BLANK_MIN_DUR = 0.5&amp;#10;BLANK_MAX_DUR = 0.5&amp;#10;&amp;#10;# how much time (in seconds), after&amp;#10;# experimenter has played attention grabbing sound,&amp;#10;# must pass before experimenter is allowed to play&amp;#10;# a new attention grabbing sound&amp;#10;ATT_SOUND_COOLDOWN_TIME = 2&amp;#10;&amp;#10;## pause video specifications ##&amp;#10;PVIDEO_WIDTH_DEG=28&amp;#10;# the width/height aspect ratio is the same for all pause videos&amp;#10;# being used, 1920(width):1080(height).&amp;#10;PVIDEO_HEIGHT_DEG=1080/1920 * PVIDEO_WIDTH_DEG&amp;#10;&amp;#10;## end pause video specifications ##&amp;#10;&amp;#10;## attention grabber manipulations ##&amp;#10;# --- pulsating attention grabber:&amp;#10;# period, ie time spent on a whole&amp;#10;# shrink/grow cycle&amp;#10;PULSE_PERIOD = 1&amp;#10;# relative amplitude - for example, 0.2&amp;#10;# means that the attention grabber will&amp;#10;# grow to 120% and shrink to 80%,&amp;#10;# 0.3 -&gt; 130%/70%&amp;#10;PULSE_AMP = 0.4&amp;#10;# --- flickering attention grabber:&amp;#10;# period, ie time spent on a whole&amp;#10;# fade-out/fade-in cycle&amp;#10;FLICKER_PERIOD = 0.7&amp;#10;# least opacity - for example, 0.5&amp;#10;# means that the attention grabber will&amp;#10;# fade out to 50% opacity/'visibility',&amp;#10;# then fade back in again to 100%&amp;#10;FLICKER_LEAST_OPACITY = 0.5&amp;#10;# --- bouncing attention grabber:&amp;#10;# period, ie time spent on a whole&amp;#10;# 'jump', going up/down and back to&amp;#10;# original position&amp;#10;BOUNCE_PERIOD = 1&amp;#10;# height, ie how many degrees (unless&amp;#10;# experiment unit is changed to something&amp;#10;# else than degree) the attention grabber&amp;#10;# should move up/down&amp;#10;BOUNCE_HEIGHT = 0.5&amp;#10;&amp;#10;## end attention grabber manipulations ##&amp;#10;&amp;#10;## messages to participant ##&amp;#10;# load screen message.&amp;#10;# &quot;Please wait for a bit...&quot;&amp;#10;LOAD_MESSAGE = (&amp;#10;    &quot;Vänligen vänta lite...&quot;&amp;#10;)&amp;#10;# end screen message.&amp;#10;# &quot;With that, the experiment is finished.&quot;&amp;#10;# &quot;Thank you!&quot;&amp;#10;END_MESSAGE = (&amp;#10;    &quot;Nu är experimentet klart.\n&quot;&amp;#10;    &quot;Tack!&quot;&amp;#10;)&amp;#10;&amp;#10;# text height (in degrees) for messages to participant&amp;#10;MESSAGE_TXT_HEIGHT = 1&amp;#10;&amp;#10;## end messages to participant ##&amp;#10;&amp;#10;### END SET CONSTANTS ###" valType="extendedCode"/>
        <Param name="Before JS Experiment" updates="constant" val="/* Syntax Error: Fix Python code */" valType="extendedCode"/>
        <Param name="Begin Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin JS Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="Code Type" updates="None" val="Py" valType="str"/>
        <Param name="Each Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="Each JS Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="End Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="End Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="disabled" updates="None" val="False" valType="bool"/>
        <Param name="name" updates="None" val="code_setup" valType="code"/>
      </CodeComponent>
      <CodeComponent name="code_eyetracker_setup">
        <Param name="Before Experiment" updates="constant" val="# load necessary libraries&amp;#10;from psychopy.iohub import launchHubServer&amp;#10;&amp;#10;# set constant that specifies if the eyetracker&amp;#10;# is to be 'mocked', ie the mouse is used instead&amp;#10;# of an actual eyetracker&amp;#10;MOCK_ON = False&amp;#10;" valType="extendedCode"/>
        <Param name="Before JS Experiment" updates="constant" val="import {launchHubServer} from 'psychopy/iohub';&amp;#10;var MOCK_ON;&amp;#10;MOCK_ON = false;&amp;#10;" valType="extendedCode"/>
        <Param name="Begin Experiment" updates="constant" val="# check if there is a pre-existing 'events.hdf5' file,&amp;#10;# meaning that a previous experiment run did&amp;#10;# not finish (since at the end of the experiment,&amp;#10;# the 'events.hdf5' file is renamed and moved to the&amp;#10;# data directory).&amp;#10;if os.path.isfile('events.hdf5'):&amp;#10;    # make sure that the 'incomplete_hdf5_files'&amp;#10;    # subdirectory in 'data' exists, and otherwise &amp;#10;    # create it&amp;#10;    inc_dpath = os.path.join('data', 'incomplete_hdf5_files')&amp;#10;    if not os.path.isdir(inc_dpath):&amp;#10;        os.mkdir(inc_dpath)&amp;#10;    # generate a random identifier number/filename for&amp;#10;    # the file to be renamed with&amp;#10;    rand_num = randint(1, 10000000)&amp;#10;    inc_fname = f'inc_{rand_num}.hdf5'&amp;#10;    # rename the file and move it to&amp;#10;    # 'data/incomplete_hdf5_files' directory&amp;#10;    os.rename(&amp;#10;        'events.hdf5',&amp;#10;        os.path.join(inc_dpath, inc_fname)&amp;#10;     )&amp;#10;&amp;#10;# specify name of YAML config file, located in the&amp;#10;# same directory as the .psyexp file.&amp;#10;# note that if 'mocking' is turned on ('Before&amp;#10;# Experiment' tab), the computer's mouse is&amp;#10;# used instead of an actual eyetracker.&amp;#10;if MOCK_ON:&amp;#10;    config_file = 'MOUSE_eyetracker_config.yaml'&amp;#10;else:&amp;#10;    config_file = 'real_eyetracker_config.yaml'&amp;#10;&amp;#10;# specify name of eye tracker (ie the name&amp;#10;# specified in the YAML file)&amp;#10;eye_tracker_name = 'tracker'&amp;#10;&amp;#10;# form file path to save eyetracker&amp;#10;# (ioHub/HDF5) data to&amp;#10;# (note that the file format, '.hdf5',&amp;#10;# is __not__ to be included in this file&amp;#10;# path, since ioHub automatically appends &amp;#10;# '.hdf5' to the passed file path&amp;#10;new_hdf5_path = os.path.join(&amp;#10;        'data',&amp;#10;        '{}_{}_{}_hdf5'.format(expInfo['participant'],&amp;#10;            expInfo['expName'],&amp;#10;            expInfo['date'])&amp;#10;)&amp;#10;&amp;#10;# attempt to connect to device specified in&amp;#10;# YAML config file&amp;#10;io_connection = launchHubServer(&amp;#10;    iohub_config_name=config_file,&amp;#10;    experiment_code='infant_audiovisual_eyetracking',&amp;#10;    datastore_name=new_hdf5_path,&amp;#10;    window=win&amp;#10;)&amp;#10;&amp;#10;# check if can get details for eye tracker device,&amp;#10;# and otherwise quit&amp;#10;if io_connection.getDevice(eye_tracker_name):&amp;#10;    # assign the tracker to a variable to make&amp;#10;    # it easier to reference&amp;#10;    eye_tracker = io_connection.getDevice(eye_tracker_name)&amp;#10;else:&amp;#10;    print(&amp;#10;        f&quot;Could not connect to eye tracker '{eye_tracker_name}', is it on?&quot;&amp;#10;        &quot; Quitting...&quot;&amp;#10;    )&amp;#10;    core.quit()&amp;#10;" valType="extendedCode"/>
        <Param name="Begin JS Experiment" updates="constant" val="/* Syntax Error: Fix Python code */" valType="extendedCode"/>
        <Param name="Begin JS Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="Code Type" updates="None" val="Auto-&gt;JS" valType="str"/>
        <Param name="Each Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="Each JS Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="End Experiment" updates="constant" val="# close io connection&amp;#10;io_connection.quit()" valType="extendedCode"/>
        <Param name="End JS Experiment" updates="constant" val="io_connection.quit();&amp;#10;" valType="extendedCode"/>
        <Param name="End JS Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="End Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="disabled" updates="None" val="False" valType="bool"/>
        <Param name="name" updates="None" val="code_eyetracker_setup" valType="code"/>
      </CodeComponent>
    </Routine>
    <Routine name="start_screen">
      <TextComponent name="text_start">
        <Param name="color" updates="constant" val="black" valType="color"/>
        <Param name="colorSpace" updates="constant" val="rgb" valType="str"/>
        <Param name="contrast" updates="constant" val="1" valType="num"/>
        <Param name="disabled" updates="None" val="False" valType="bool"/>
        <Param name="durationEstim" updates="None" val="" valType="num"/>
        <Param name="flip" updates="constant" val="None" valType="str"/>
        <Param name="font" updates="constant" val="Open Sans" valType="str"/>
        <Param name="languageStyle" updates="None" val="LTR" valType="str"/>
        <Param name="letterHeight" updates="constant" val="1" valType="num"/>
        <Param name="name" updates="None" val="text_start" valType="code"/>
        <Param name="opacity" updates="constant" val="" valType="num"/>
        <Param name="ori" updates="constant" val="0" valType="num"/>
        <Param name="pos" updates="constant" val="(0, 0)" valType="list"/>
        <Param name="saveStartStop" updates="None" val="False" valType="bool"/>
        <Param name="startEstim" updates="None" val="" valType="num"/>
        <Param name="startType" updates="None" val="time (s)" valType="str"/>
        <Param name="startVal" updates="None" val="0.0" valType="num"/>
        <Param name="stopType" updates="None" val="duration (s)" valType="str"/>
        <Param name="stopVal" updates="constant" val="2" valType="num"/>
        <Param name="syncScreenRefresh" updates="None" val="True" valType="bool"/>
        <Param name="text" updates="constant" val="Nu börjar det!" valType="str"/>
        <Param name="units" updates="None" val="deg" valType="str"/>
        <Param name="wrapWidth" updates="constant" val="" valType="num"/>
      </TextComponent>
      <CodeComponent name="code_eyetracker_startrecording">
        <Param name="Before Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Before JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin JS Routine" updates="constant" val="/* Syntax Error: Fix Python code */" valType="extendedCode"/>
        <Param name="Begin Routine" updates="constant" val="# tell ioHub how to label the oncoming series of eye tracker&amp;#10;# data&amp;#10;io_connection.sendMessageEvent('start_recording')&amp;#10;&amp;#10;# instruct eye tracker to stream eye data to PsychoPy's ioHub&amp;#10;# (meaning the data will be later stored in the participant's&amp;#10;# HDF5 data file)&amp;#10;eye_tracker.setRecordingState(True)&amp;#10;" valType="extendedCode"/>
        <Param name="Code Type" updates="None" val="Auto-&gt;JS" valType="str"/>
        <Param name="Each Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="Each JS Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="End Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="End Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="disabled" updates="None" val="False" valType="bool"/>
        <Param name="name" updates="None" val="code_eyetracker_startrecording" valType="code"/>
      </CodeComponent>
    </Routine>
    <Routine name="end_screen">
      <TextComponent name="text_end">
        <Param name="color" updates="constant" val="black" valType="color"/>
        <Param name="colorSpace" updates="constant" val="rgb" valType="str"/>
        <Param name="contrast" updates="constant" val="1" valType="num"/>
        <Param name="disabled" updates="None" val="False" valType="bool"/>
        <Param name="durationEstim" updates="None" val="" valType="num"/>
        <Param name="flip" updates="constant" val="None" valType="str"/>
        <Param name="font" updates="constant" val="Open Sans" valType="str"/>
        <Param name="languageStyle" updates="None" val="LTR" valType="str"/>
        <Param name="letterHeight" updates="constant" val="MESSAGE_TXT_HEIGHT" valType="num"/>
        <Param name="name" updates="None" val="text_end" valType="code"/>
        <Param name="opacity" updates="constant" val="" valType="num"/>
        <Param name="ori" updates="constant" val="0" valType="num"/>
        <Param name="pos" updates="constant" val="(0, 0)" valType="list"/>
        <Param name="saveStartStop" updates="None" val="True" valType="bool"/>
        <Param name="startEstim" updates="None" val="" valType="num"/>
        <Param name="startType" updates="None" val="time (s)" valType="str"/>
        <Param name="startVal" updates="None" val="0.0" valType="num"/>
        <Param name="stopType" updates="None" val="duration (s)" valType="str"/>
        <Param name="stopVal" updates="constant" val="5.0" valType="num"/>
        <Param name="syncScreenRefresh" updates="None" val="True" valType="bool"/>
        <Param name="text" updates="constant" val="$END_MESSAGE" valType="str"/>
        <Param name="units" updates="None" val="deg" valType="str"/>
        <Param name="wrapWidth" updates="constant" val="" valType="num"/>
      </TextComponent>
      <CodeComponent name="code_eyetracker_stoprecording">
        <Param name="Before Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Before JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin JS Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="Code Type" updates="None" val="Auto-&gt;JS" valType="str"/>
        <Param name="Each Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="Each JS Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="End Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Routine" updates="constant" val="eye_tracker.setRecordingState(false);&amp;#10;" valType="extendedCode"/>
        <Param name="End Routine" updates="constant" val="# stop eye tracker recording&amp;#10;eye_tracker.setRecordingState(False)" valType="extendedCode"/>
        <Param name="disabled" updates="None" val="False" valType="bool"/>
        <Param name="name" updates="None" val="code_eyetracker_stoprecording" valType="code"/>
      </CodeComponent>
    </Routine>
    <Routine name="trial_second_soundrepeat">
      <CodeComponent name="code_trial2srep">
        <Param name="Before Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Before JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin JS Experiment" updates="constant" val="img_ls = [];&amp;#10;pos_ls = [[0, IMG_CIRCLE_RADIUS], [IMG_CIRCLE_RADIUS, 0], [0, (- IMG_CIRCLE_RADIUS)], [(- IMG_CIRCLE_RADIUS), 0]];&amp;#10;for (var i = 0, _pj_a = 4; (i &lt; _pj_a); i += 1) {&amp;#10;    new visual.ImageStim({&quot;win&quot;: win, &quot;name&quot;: &quot;image&quot;, &quot;image&quot;: null, &quot;mask&quot;: null, &quot;ori&quot;: 0, &quot;pos&quot;: [0, 0.5], &quot;size&quot;: [0.3, 0.3], &quot;color&quot;: [1, 1, 1], &quot;colorSpace&quot;: &quot;rgb&quot;, &quot;opacity&quot;: 1, &quot;flipHoriz&quot;: false, &quot;flipVert&quot;: false, &quot;texRes&quot;: 512, &quot;interpolate&quot;: true, &quot;depth&quot;: 0.0});&amp;#10;}&amp;#10;" valType="extendedCode"/>
        <Param name="Begin JS Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin Routine" updates="constant" val="# calculate start times/delays and durations of stimuli&amp;#10;# attention grabber&amp;#10;att_grab_extra_startdelay = random_20thsec((ATT_GRAB_LATEST - ATT_GRAB_EARLIEST)*20)&amp;#10;att_grab_start = ATT_GRAB_EARLIEST + att_grab_extra_startdelay&amp;#10;# auditory stimulus&amp;#10;audio_extra_delay = random_20thsec((AUDITORY_MAX_DELAY - AUDITORY_MIN_DELAY)*20)&amp;#10;audio_delay = AUDITORY_MIN_DELAY + audio_extra_delay&amp;#10;# visual stimuli&amp;#10;visual_extra_delay = random_20thsec((VISUAL_MAX_DELAY - VISUAL_MIN_DELAY)*20)&amp;#10;visual_delay = VISUAL_MIN_DELAY + visual_extra_delay&amp;#10;visual_extra_dur = random_20thsec((VISUAL_MAX_DUR - VISUAL_MIN_DUR)*20)&amp;#10;visual_dur = VISUAL_MIN_DUR + visual_extra_dur&amp;#10;# 'blank' period after visual stimuli have been removed&amp;#10;blank_extra_dur = random_20thsec((BLANK_MAX_DUR - BLANK_MIN_DUR)*20)&amp;#10;blank_dur = BLANK_MIN_DUR + blank_extra_dur&amp;#10;&amp;#10;# booleans indicating if the different components are &amp;#10;# active/started (ie shown/played)&amp;#10;att_active = False&amp;#10;audio_started = False&amp;#10;visual_started = False&amp;#10;visual_active = False&amp;#10;&amp;#10;# boolean indicating if gaze has been directed&amp;#10;# at attention grabber&amp;#10;gaze_captured = False&amp;#10;&amp;#10;# timepoints (initialized with very high values,&amp;#10;# to avoid events being preemptively triggered)&amp;#10;# timepoint at which attention grabber starts&amp;#10;# being presented&amp;#10;att_onset_t = 9999999&amp;#10;# timepoint at which gaze is directed at&amp;#10;# attention grabber&amp;#10;gaze_capture_t = 9999999&amp;#10;# timepoint at which sound starts playing&amp;#10;audio_onset_t = 9999999&amp;#10;# timepoint at which visual stimuli start to be presented&amp;#10;visual_onset_t = 9999999&amp;#10;# timepoint at which visual stimuli stop being presented&amp;#10;visual_end_t = 9999999&amp;#10;&amp;#10;# extract original size of attention grabber,&amp;#10;# as it might change&amp;#10;att_orig_size = att_img.size&amp;#10;&amp;#10;# TODO make this position assignment guarantee&amp;#10;# that the new face image is assigned a _different_ position&amp;#10;# from where the face was in the preceding trial&amp;#10;# generate a random array of visual stimuli to use&amp;#10;img_ls = get_trial_img_ls()&amp;#10;# assign positions to the visual stimuli&amp;#10;for i, pos in enumerate(pos_ls):&amp;#10;    img_ls[i].pos = pos&amp;#10;# set trial pair's second sound stimulus volume&amp;#10;sound_trial.volume = sound_volumes[1]&amp;#10;&amp;#10;# randomly pick an attention grabber image&amp;#10;# to use for this trial&amp;#10;att_img = randchoice(att_img_ls, 1)[0]&amp;#10;# extract original size/opacity of attention grabber,&amp;#10;# as they might change (depending on the type&amp;#10;# of attention grabber animation used)&amp;#10;att_orig_size = att_img.size&amp;#10;att_orig_opacity = att_img.opacity&amp;#10;# randomly pick which attention grabber&amp;#10;# animation (function) to use&amp;#10;animater_name = randchoice(att_animater_names, 1)[0]&amp;#10;&amp;#10;# if debugging is enabled&amp;#10;if DEBUG_ON:&amp;#10;    # boolean indicating if screenshot has been taken&amp;#10;    trial_screenshot_taken = False&amp;#10;    attgrab_screenshot_taken = False&amp;#10;" valType="extendedCode"/>
        <Param name="Code Type" updates="None" val="Py" valType="str"/>
        <Param name="Each Frame" updates="constant" val="# TO DO: consider where it's appropriate to use&amp;#10;# time of next flip rather than current time (t) - &amp;#10;# eg 'visual_end_t' should probably rely on&amp;#10;# next flip time, since that makes more intuitive&amp;#10;# sense&amp;#10;#rect_background.draw()&amp;#10;&amp;#10;# time to activate attention grabber?&amp;#10;if t &gt; att_grab_start and not (att_active or gaze_captured):&amp;#10;    att_active = True&amp;#10;    att_t_start = t&amp;#10;&amp;#10;# attention grabber active?&amp;#10;if att_active:&amp;#10;    # time passed since attention grabber was activated&amp;#10;    att_passed_t = t - att_t_start&amp;#10;    # animate the attention grabber image, based on which&amp;#10;    # animation function was randomly picked in 'Begin Routine'&amp;#10;    if animater_name == 'pulsate':&amp;#10;        pulsate_image(&amp;#10;            att_img,&amp;#10;            att_passed_t,&amp;#10;            att_orig_size&amp;#10;        )&amp;#10;    elif animater_name == 'flicker':&amp;#10;        flicker_image(&amp;#10;            att_img,&amp;#10;            att_passed_t,&amp;#10;            att_orig_opacity&amp;#10;        )&amp;#10;    elif animater_name == 'bounce':&amp;#10;        bounce_image(&amp;#10;            att_img,&amp;#10;            att_passed_t,&amp;#10;            att_orig_y_coord&amp;#10;        )&amp;#10;    # show the attention grabber&amp;#10;    att_img.draw()&amp;#10;    # check if gaze (as measured by eye tracker)&amp;#10;    # is within attention grabber area&amp;#10;    gaze_pos = eye_tracker.getPosition()&amp;#10;    if gaze_pos and att_img.contains(*gaze_pos, units='pix'):&amp;#10;        att_active = False&amp;#10;        gaze_captured = True&amp;#10;        gaze_capture_t = t&amp;#10;&amp;#10;# gaze has been captured and time of delay&amp;#10;# from gaze capture-&gt;sound onset has passed?&amp;#10;if gaze_captured and not audio_started and t &gt; (gaze_capture_t + audio_delay):&amp;#10;    audio_started = True&amp;#10;    sound_trial.play()&amp;#10;    audio_onset_t = t&amp;#10;&amp;#10;# gaze has been captured and time of delay&amp;#10;# from sound onset-&gt;visual stimul onset has passed?&amp;#10;if audio_started and not visual_started and t &gt; (audio_onset_t + visual_delay):&amp;#10;    visual_active = True&amp;#10;    visual_started = True&amp;#10;    visual_onset_t = t&amp;#10;&amp;#10;# visual stimuli are active?&amp;#10;if visual_active:&amp;#10;    for img in img_ls:&amp;#10;        img.draw()&amp;#10;    # visual stimuli have been shown for&amp;#10;    # set duration?&amp;#10;    if (t - visual_onset_t) &gt;= visual_dur:&amp;#10;        visual_active = False&amp;#10;        visual_end_t = t&amp;#10;&amp;#10;# end blank screen has been shown for its&amp;#10;# intended duration?&amp;#10;if t &gt; (visual_end_t + blank_dur):&amp;#10;    # end trial&amp;#10;    continueRoutine = False&amp;#10;&amp;#10;# if debugging is enabled&amp;#10;if DEBUG_ON:&amp;#10;    # draw areas of interest outlines&amp;#10;    att_aoi_comp.draw()&amp;#10;    for trial_aoi in trial_aoi_components:&amp;#10;        trial_aoi.draw()&amp;#10;    # take screenshot of trial at attention grabber onset&amp;#10;    if not attgrab_screenshot_taken and att_active:&amp;#10;        win.getMovieFrame(buffer='back')&amp;#10;        win.saveMovieFrames(f'exp_snapshots/att_grab_{trial_counter}.png', codec='png')&amp;#10;        attgrab_screenshot_taken = True&amp;#10;    # take screenshot of trial at visual stimuli onset&amp;#10;    if not trial_screenshot_taken and visual_active:&amp;#10;        win.getMovieFrame(buffer='back')&amp;#10;        win.saveMovieFrames(f'exp_snapshots/trial_{trial_counter}.png', codec='png')&amp;#10;        trial_screenshot_taken = True&amp;#10;" valType="extendedCode"/>
        <Param name="Each JS Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="End Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="End Routine" updates="constant" val="# assign attention grabber image component its original size/opacity,&amp;#10;# in case they were manipulated for animation (depends on what&amp;#10;# type of animation was used)&amp;#10;att_img.size = att_orig_size&amp;#10;att_img.opacity = att_orig_opacity&amp;#10;att_img.pos = (att_img.pos[0], att_orig_y_coord)&amp;#10;&amp;#10;# increment trial counter&amp;#10;trial_counter += 1" valType="extendedCode"/>
        <Param name="disabled" updates="None" val="False" valType="bool"/>
        <Param name="name" updates="None" val="code_trial2srep" valType="code"/>
      </CodeComponent>
      <TextComponent name="text_trial2srep_timekeeper">
        <Param name="color" updates="constant" val="white" valType="str"/>
        <Param name="colorSpace" updates="constant" val="rgb" valType="str"/>
        <Param name="contrast" updates="constant" val="1" valType="num"/>
        <Param name="disabled" updates="None" val="False" valType="bool"/>
        <Param name="durationEstim" updates="None" val="" valType="code"/>
        <Param name="flip" updates="constant" val="" valType="str"/>
        <Param name="font" updates="constant" val="Arial" valType="str"/>
        <Param name="languageStyle" updates="None" val="LTR" valType="str"/>
        <Param name="letterHeight" updates="constant" val="0.1" valType="code"/>
        <Param name="name" updates="None" val="text_trial2srep_timekeeper" valType="code"/>
        <Param name="opacity" updates="constant" val="1" valType="code"/>
        <Param name="ori" updates="constant" val="0" valType="code"/>
        <Param name="pos" updates="constant" val="(0, 0)" valType="code"/>
        <Param name="saveStartStop" updates="None" val="True" valType="bool"/>
        <Param name="startEstim" updates="None" val="" valType="code"/>
        <Param name="startType" updates="None" val="time (s)" valType="str"/>
        <Param name="startVal" updates="None" val="0.0" valType="code"/>
        <Param name="stopType" updates="None" val="duration (s)" valType="str"/>
        <Param name="stopVal" updates="constant" val="" valType="code"/>
        <Param name="syncScreenRefresh" updates="None" val="True" valType="bool"/>
        <Param name="text" updates="constant" val="" valType="extendedStr"/>
        <Param name="units" updates="None" val="from exp settings" valType="str"/>
        <Param name="wrapWidth" updates="constant" val="" valType="code"/>
      </TextComponent>
      <CodeComponent name="code_eyetracker_message_2srep">
        <Param name="Before Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Before JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin JS Routine" updates="constant" val="/* Syntax Error: Fix Python code */" valType="extendedCode"/>
        <Param name="Begin Routine" updates="constant" val="# send 'trial start' message to ioHub/the eye tracker &amp;#10;# that will be included in the next eye tracker &amp;#10;# data record&amp;#10;io_connection.sendMessageEvent('trial start (2nd sound repeat)')&amp;#10;" valType="extendedCode"/>
        <Param name="Code Type" updates="None" val="Auto-&gt;JS" valType="str"/>
        <Param name="Each Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="Each JS Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="End Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Routine" updates="constant" val="/* Syntax Error: Fix Python code */" valType="extendedCode"/>
        <Param name="End Routine" updates="constant" val="# send 'trial end' message to ioHub/the eye tracker &amp;#10;# that will be included in the next eye tracker &amp;#10;# data record&amp;#10;io_connection.sendMessageEvent('trial end (2nd sound repeat)')&amp;#10;" valType="extendedCode"/>
        <Param name="disabled" updates="None" val="False" valType="bool"/>
        <Param name="name" updates="None" val="code_eyetracker_message_2srep" valType="code"/>
      </CodeComponent>
    </Routine>
    <Routine name="trial">
      <TextComponent name="text_trial1_timekeeper">
        <Param name="color" updates="constant" val="white" valType="str"/>
        <Param name="colorSpace" updates="constant" val="rgb" valType="str"/>
        <Param name="contrast" updates="constant" val="1" valType="num"/>
        <Param name="disabled" updates="None" val="False" valType="bool"/>
        <Param name="durationEstim" updates="None" val="" valType="code"/>
        <Param name="flip" updates="constant" val="" valType="str"/>
        <Param name="font" updates="constant" val="Arial" valType="str"/>
        <Param name="languageStyle" updates="None" val="LTR" valType="str"/>
        <Param name="letterHeight" updates="constant" val="1" valType="code"/>
        <Param name="name" updates="None" val="text_trial1_timekeeper" valType="code"/>
        <Param name="opacity" updates="constant" val="1" valType="code"/>
        <Param name="ori" updates="constant" val="0" valType="code"/>
        <Param name="pos" updates="constant" val="(0, 0)" valType="code"/>
        <Param name="saveStartStop" updates="None" val="False" valType="bool"/>
        <Param name="startEstim" updates="None" val="" valType="code"/>
        <Param name="startType" updates="None" val="time (s)" valType="str"/>
        <Param name="startVal" updates="None" val="0.0" valType="code"/>
        <Param name="stopType" updates="None" val="duration (s)" valType="str"/>
        <Param name="stopVal" updates="constant" val="" valType="code"/>
        <Param name="syncScreenRefresh" updates="None" val="True" valType="bool"/>
        <Param name="text" updates="constant" val="" valType="extendedStr"/>
        <Param name="units" updates="None" val="from exp settings" valType="str"/>
        <Param name="wrapWidth" updates="constant" val="" valType="code"/>
      </TextComponent>
      <CodeComponent name="code_eyetracker_message_trial">
        <Param name="Before Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Before JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin JS Routine" updates="constant" val="io_connection.sendMessageEvent(`trial ${trial_counter} start`);&amp;#10;" valType="extendedCode"/>
        <Param name="Begin Routine" updates="constant" val="# send 'trial start' message to ioHub (ie the eye tracker), &amp;#10;# that will be included in the next eye tracker &amp;#10;# data record&amp;#10;# NOTE that there are also some 'send message to ioHub' code&amp;#10;# scattered throughout the 'code_trial' code snippet's&amp;#10;# 'Each Frame' tab&amp;#10;io_connection.sendMessageEvent(f'exp1 trial {trial_counter+1} start')&amp;#10;" valType="extendedCode"/>
        <Param name="Code Type" updates="None" val="Py" valType="str"/>
        <Param name="Each Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="Each JS Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="End Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Routine" updates="constant" val="/* Syntax Error: Fix Python code */" valType="extendedCode"/>
        <Param name="End Routine" updates="constant" val="# send 'trial end' message to ioHub/the eye tracker &amp;#10;# that will be included in the next eye tracker &amp;#10;# data record&amp;#10;io_connection.sendMessageEvent(f'exp1 trial {trial_counter+1} end')&amp;#10;" valType="extendedCode"/>
        <Param name="disabled" updates="None" val="False" valType="bool"/>
        <Param name="name" updates="None" val="code_eyetracker_message_trial" valType="code"/>
      </CodeComponent>
      <CodeComponent name="code_trial">
        <Param name="Before Experiment" updates="constant" val="# import the pandas package (which is already being &amp;#10;# loaded and used by PsychoPy in the background, &amp;#10;# so there's no performance loss)&amp;#10;# in order to be able to read and process&amp;#10;# CSV files with stimuli specifications&amp;#10;import pandas as pd&amp;#10;# read in image/audio stimuli specifications&amp;#10;# and put them in pandas dataframes&amp;#10;# 'in-trial' visual stimuli&amp;#10;img_df = pd.read_csv(IMG_SPEC_PATH)&amp;#10;# 'in-trial' audio stimuli&amp;#10;audio_df = pd.read_csv(AUDIO_SPEC_PATH)&amp;#10;# attention grabbing audio stimuli&amp;#10;att_audio_df = pd.read_csv(ATT_AUDIO_SPEC_PATH)&amp;#10;# attention grabber image specifications&amp;#10;att_img_df = pd.read_csv(ATT_IMG_SPEC_PATH)&amp;#10;&amp;#10;def get_trial_img_ls():&amp;#10;    &quot;&quot;&quot;&amp;#10;    Randomly grabs&amp;#10;    1 social image stimulus component&amp;#10;    1 geometrical shape stimulus component&amp;#10;    1 natural object stimulus component&amp;#10;    1 man-made object stimulus component&amp;#10;    and returns them as a randomly-ordered&amp;#10;    4-element numpy array.&amp;#10;    Note that this function relies on the lists `img_ls_social`&amp;#10;    and `img_ls_nonsocial` already having been created before&amp;#10;    this function is called. See the 'Begin Experiment' tab,&amp;#10;    where these two lists are created.&amp;#10;    &quot;&quot;&quot;&amp;#10;    # 'pop off' an image from each category's list of images. this means&amp;#10;    # that, for each category, the last image is 'plucked' from the end of &amp;#10;    # the list of images, and 'given to' the image variable to the left&amp;#10;    # of the '=' operator/sign. doing this ensures that once an image has&amp;#10;    # been fetched for use in one trial, it won't be used for another&amp;#10;    soc_img = img_ls_social.pop()&amp;#10;    geo_img = img_ls_geo.pop()&amp;#10;    manm_img = img_ls_manm.pop()&amp;#10;    nat_img = img_ls_nat.pop()&amp;#10;    # combine the four images into a list&amp;#10;    trial_img_ls = [soc_img, geo_img, manm_img, nat_img]&amp;#10;    return trial_img_ls&amp;#10;&amp;#10;def calc_width_height(orig_w, orig_h, max_w, max_h):&amp;#10;    &quot;&quot;&quot;&amp;#10;    Calculates PsychoPy image stimulus width/height specifications based on&amp;#10;    original image's width/height (orig_w, orig_h) and the maximum width/height&amp;#10;    in PsychoPy.&amp;#10;    &quot;&quot;&quot;&amp;#10;    w_ratio = max_w / orig_w&amp;#10;    h_ratio = max_h / orig_h&amp;#10;    if (w_ratio &lt; h_ratio):&amp;#10;        return w_ratio * orig_w, w_ratio * orig_h&amp;#10;    else:&amp;#10;        return h_ratio * orig_w, h_ratio * orig_h&amp;#10;&amp;#10;def random_20thsec(max_n_20ths):&amp;#10;    &quot;&quot;&quot;&amp;#10;    Randomly picks a number in the range [0, &lt;max_n_20ths&gt;*0.05],&amp;#10;    in order to randomly pick a duration using increments of 0.05s.&amp;#10;    Note that the range is inclusive on both ends, ie if you pass&amp;#10;    in 4 you might get 0, 0.05, 0.10, 0.15 or 0.2 back.&amp;#10;    &quot;&quot;&quot;&amp;#10;    num_increments_0p05 = np.random.randint(max_n_20ths+1)&amp;#10;    return num_increments_0p05/20&amp;#10;&amp;#10;def generate_img_components_from_df(&amp;#10;    df, &amp;#10;    max_width=IMG_MAX_WIDTH, &amp;#10;    max_height=IMG_MAX_HEIGHT,&amp;#10;    img_category=&quot;default&quot;,&amp;#10;    with_lin_interpolation=True&amp;#10;):&amp;#10;    &quot;&quot;&quot;&amp;#10;    (used in Begin Experiment) Takes in a data frame (df), describing images, with the following columns:&amp;#10;        - 'width', specifying image's original width&amp;#10;        - 'height', specifying image's original height&amp;#10;        - 'file_path', specifying image's file path (relative to .psyexp file, or absolute path)&amp;#10;    Returns a list of PsychoPy ImageStim instances, ie image components, which are&amp;#10;    based on the passed image widths/heights/paths, except down/up-sized based on&amp;#10;    the experiment's maximum allowed image width/heights (IMG_MAX_WIDTH/IMG_MAX_HEIGHT).&amp;#10;    &amp;#10;    Optionally, a (string) img_category argument can be passed to this &amp;#10;    function (defaults to &quot;default&quot;), which will be set as the 'img_category'&amp;#10;    property of all resulting image components. Eg if 'manmade' is passed as&amp;#10;    an argument, then all produced image components will have an img_category &amp;#10;    property with value 'manmade'.&amp;#10;    &amp;#10;    'with_lin_interpolation' is used to specify if linear interpolation should be&amp;#10;    used. This is necessary for photo-based image components (see 'Begin Experiment' &amp;#10;    for more information on a bug that has to be handled, however) but should not be used&amp;#10;    for simpler images, such as the attention grabbers which are non-photo-based.&amp;#10;    &quot;&quot;&quot;&amp;#10;    img_ls = []&amp;#10;    # loop over all rows of the image dataframe,&amp;#10;    # creating an image component with each&amp;#10;    # row's specifications, and appending it to&amp;#10;    # the list&amp;#10;    for row_ind in range(len(df)):&amp;#10;        row = df.iloc[row_ind]&amp;#10;        width_height = calc_width_height(&amp;#10;            row['width'],&amp;#10;            row['height'],&amp;#10;            max_width,&amp;#10;            max_height&amp;#10;        )&amp;#10;        new_img = visual.ImageStim(&amp;#10;            win=win,&amp;#10;            name=f'image_social_{row_ind}', &amp;#10;            image=df.iloc[row_ind]['file_path'], &amp;#10;            mask=None,&amp;#10;            ori=0, &amp;#10;            pos=(0, 0), &amp;#10;            size=width_height,&amp;#10;            color=[1,1,1], &amp;#10;            colorSpace='rgb', &amp;#10;            opacity=1,&amp;#10;            flipHoriz=False, &amp;#10;            flipVert=False,&amp;#10;            texRes=2048, &amp;#10;            interpolate=with_lin_interpolation, &amp;#10;            depth=0.0&amp;#10;        )&amp;#10;        new_img.color='white'&amp;#10;        if img_category:&amp;#10;            new_img.img_category = img_category&amp;#10;        img_ls.append(new_img)&amp;#10;    # shuffle (randomly order) &amp;#10;    # the generated list of image components,&amp;#10;    # to make sure that each trial run gets a&amp;#10;    # completely random order of images&amp;#10;    shuffle(img_ls)&amp;#10;    # return the generated list of image components&amp;#10;    return img_ls&amp;#10;&amp;#10;def generate_position_indices(previous_indices = []):&amp;#10;    &quot;&quot;&quot;&amp;#10;    Generates visual stimuli position indices. eg 0 might correspond to&amp;#10;    top left, 1 top right, etc. The interpretation of the indices&amp;#10;    depends on how the generated values are used, since this&amp;#10;    function doesn't know about the actual positions, it just generates&amp;#10;    numbers. See the 'Begin Routine' tab for code where this function is used.&amp;#10;    &amp;#10;    As the first argument, 'previous_positions', this function takes in&amp;#10;    a list of position indices used in the previous (if any) trial.&amp;#10;    The function ensures that the new indices it returns doesn't match&amp;#10;    the previous trials' at any point in the list. So for example,&amp;#10;    if this function is passed [0, 2, 3, 1], then it could return&amp;#10;    [1, 3, 0, 2], but not [1, 4, 2, 3], since the 1 would be at the start&amp;#10;    of the list in both 'previous_indices' and the in the returned list.&amp;#10;    &quot;&quot;&quot;&amp;#10;    # if the passed list of previous indices is empty (ie there hasn't been a&amp;#10;    # previous trial), fill it up with numbers that won't cause a collision&amp;#10;    if not previous_indices:&amp;#10;        previous_indices = [-1, -1, -1, -1]&amp;#10;    # get the length of the 'previous_indices' list, to make sure&amp;#10;    # that as many indices are generated here&amp;#10;    num_inds = len(previous_indices)&amp;#10;    # generate a list of the possible index numbers&amp;#10;    possible_indices = list(range(num_inds))&amp;#10;    &amp;#10;    found_valid_list = False&amp;#10;    # keep generating lists of position indices until one that doesn't&amp;#10;    # 'collide' with the passed previous_indices list is found&amp;#10;    while not found_valid_list:&amp;#10;        collides = False&amp;#10;        # randomly reorder the list of possible index numbers&amp;#10;        shuffle(possible_indices)&amp;#10;        # check for any collisions&amp;#10;        for i in range(num_inds):&amp;#10;            if possible_indices[i] == previous_indices[i]:&amp;#10;                collides = True&amp;#10;        if not collides:&amp;#10;            found_valid_list = True&amp;#10;    return possible_indices&amp;#10;&amp;#10;def assign_pseudorandom_volume(sound_component):&amp;#10;    &quot;&quot;&quot;&amp;#10;    Takes in a sound component. If the component's volume is set to 0&amp;#10;    (or some other value that isn't the specified lower/higher volume),&amp;#10;    this function randomly chooses between the experiment's lower and &amp;#10;    higher (set in the 'setup' code snippet) volumes and assigns the &amp;#10;    sound component the chosen volume. If the component's volume is set&amp;#10;    to the lower or higher volume already, the volume is updated to&amp;#10;    the other of the two. eg if the passed component has the higher&amp;#10;    volume set, this function updates the component's volume to the&amp;#10;    lower level.&amp;#10;    &quot;&quot;&quot;&amp;#10;    sound_option_ls = [TRIAL_AUDIO_LOW, TRIAL_AUDIO_HIGH]&amp;#10;    if sound_component.volume == TRIAL_AUDIO_LOW:&amp;#10;        sound_component.volume = TRIAL_AUDIO_HIGH&amp;#10;    elif sound_component.volume == TRIAL_AUDIO_HIGH:&amp;#10;        sound_component.volume = TRIAL_AUDIO_LOW&amp;#10;    else:&amp;#10;        # use the numpy package's random.choice function (given the&amp;#10;        # 'alias'/'nickname' randchoice by PsychoPy) to randomly&amp;#10;        # select low or high volume&amp;#10;        sound_component.volume = randchoice(sound_option_ls, 1)[0]&amp;#10;" valType="extendedCode"/>
        <Param name="Before JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin Experiment" updates="constant" val="# draw a 'waiting screen' image+text and flip the window, to give the&amp;#10;# participant something to look at while stimuli are being loaded&amp;#10;load_screen_img = visual.ImageStim(&amp;#10;    win=win,&amp;#10;    name=f'load_screen_image', &amp;#10;    image='stimuli/visual/load_screen_elephant.png', &amp;#10;    mask=None,&amp;#10;    ori=0, &amp;#10;    pos=(0, 0), &amp;#10;    size=(LOAD_IMG_WIDTH, LOAD_IMG_HEIGHT),&amp;#10;    color=[1,1,1],&amp;#10;    colorSpace='rgb',&amp;#10;    opacity=1,&amp;#10;    flipHoriz=False, &amp;#10;    flipVert=False,&amp;#10;    texRes=2048,&amp;#10;    interpolate=True,&amp;#10;    depth=0.0&amp;#10;)&amp;#10;load_screen_text = visual.TextStim(win=win, name='text_start',&amp;#10;    text=LOAD_MESSAGE,&amp;#10;    font='Open Sans',&amp;#10;    units='deg', pos=(0, -LOAD_IMG_HEIGHT/2 - MESSAGE_TXT_HEIGHT), &amp;#10;    height=MESSAGE_TXT_HEIGHT, wrapWidth=None, ori=0.0, &amp;#10;    color='black', colorSpace='rgb', opacity=None, &amp;#10;    languageStyle='LTR',&amp;#10;    depth=0.0&amp;#10;);&amp;#10;&amp;#10;load_screen_img.draw()&amp;#10;load_screen_text.draw()&amp;#10;win.flip()&amp;#10;&amp;#10;# -- pre-load social image stimuli by creating a list of&amp;#10;# image stimulus components --&amp;#10;# extract the subset of rows in the dataframe&amp;#10;# which pertain to social stimuli&amp;#10;social_img_df = img_df[img_df.type=='social']&amp;#10;# use function defined in the 'Before Experiment' tab to generate&amp;#10;# a list of PsychoPy image components based on the social images&amp;#10;img_ls_social = generate_img_components_from_df(&amp;#10;    social_img_df,&amp;#10;    img_category=&quot;social&quot;&amp;#10;)&amp;#10;&amp;#10;# pre-load non-social images, in the same way as was done for social images,&amp;#10;# repeating the steps for each type of non-social images&amp;#10;img_ls_geo = generate_img_components_from_df(&amp;#10;    img_df[img_df.type=='geometric'],&amp;#10;    img_category=&quot;geometric&quot;&amp;#10;)&amp;#10;img_ls_manm = generate_img_components_from_df(&amp;#10;    img_df[img_df.type=='manmade'],&amp;#10;    img_category=&quot;manmade&quot;&amp;#10;)&amp;#10;img_ls_nat = generate_img_components_from_df(&amp;#10;    img_df[img_df.type=='natural'],&amp;#10;    img_category=&quot;natural&quot;&amp;#10;)&amp;#10;&amp;#10;&amp;#10;# all trial images should be at equal distances from&amp;#10;# center of screen, arranged in a 'cross-like'&amp;#10;# shape, ie they have equal x/y offsets from the&amp;#10;# vertical/horizontal center lines&amp;#10;x_offset = IMG_X_OFFSET&amp;#10;y_offset = IMG_Y_OFFSET&amp;#10;pos_ls = [&amp;#10;    (-x_offset, y_offset),&amp;#10;    (x_offset, y_offset),&amp;#10;    (-x_offset, -y_offset),&amp;#10;    (x_offset, -y_offset)&amp;#10;]&amp;#10;&amp;#10;# a variable for a list of position indices, used for assigning the above &amp;#10;# positions to trial visual stimuli. this list is updated for every trial,&amp;#10;# see the definition of 'generate_position_indices' in the 'Before&amp;#10;# Experiment' tab. the variable is initialized with a value of None &amp;#10;# to ensure that an entirely random list of indices is generated for the&amp;#10;# first trial&amp;#10;position_indices = None&amp;#10;&amp;#10;# pre-load attention grabber images by creating a list of&amp;#10;# attention grabber image components, specifying the&amp;#10;# attention image maximum width/height&amp;#10;att_img_ls = generate_img_components_from_df(&amp;#10;    att_img_df, &amp;#10;    max_width=ATT_MAX_WIDTH, &amp;#10;    max_height=ATT_MAX_WIDTH,&amp;#10;    with_lin_interpolation=False&amp;#10;)&amp;#10;&amp;#10;# pre-load sound stimuli by creating a list of&amp;#10;# sound stimulus components&amp;#10;sound_ls = []&amp;#10;for row_ind in range(len(audio_df)):&amp;#10;    new_sound = sound.Sound(&amp;#10;        audio_df.iloc[row_ind]['file_path'],&amp;#10;        name=f'sound_{row_ind}', &amp;#10;        stereo=True, &amp;#10;        hamming=True,&amp;#10;        # set a very random initial volume that it won't be equal to either&amp;#10;        # high or low volume level as specified in setup, and used in &amp;#10;        # 'assign_pseudorandom_volume' function&amp;#10;        volume=0.12345&amp;#10;    )&amp;#10;    # add 'custom' attributes (ie not one of the Sound class' normal attributes)&amp;#10;    # for keeping track of what type of sound ('social'/'non-social') this is, and&amp;#10;    # what the sex of the recorded voice is ('male'/'female'/'non-social')&amp;#10;    new_sound.sound_type = audio_df.iloc[row_ind]['type']&amp;#10;    new_sound.voice_sex = audio_df.iloc[row_ind]['sex']&amp;#10;    sound_ls.append(new_sound)&amp;#10;# add 2 None elements to the list of sounds. these represent&amp;#10;# silent trials, where no audio stimulus is to be played.&amp;#10;# (in total, there will be 8 silent trials)&amp;#10;sound_ls += [None] * 2&amp;#10;# shuffle, ie randomly reorder the list of sound components and None&amp;#10;# elements (for silent trials - see directly above).&amp;#10;# this order will be 'used twice'. first, it's used to go through&amp;#10;# all the sounds, playing each one at either high or low volume for two&amp;#10;# trials in a row (28 trials in total, including silent ones).&amp;#10;# then, the experiment will go through all sounds in the same order as&amp;#10;# before, playing them at the other volume level (eg if sound A was played&amp;#10;# at low volume during the first 'run', sound A will later be played at&amp;#10;# high volume)&amp;#10;shuffle(sound_ls)&amp;#10;&amp;#10;# pre-load attention grabbing sounds by creating a list of&amp;#10;# sound stimulus components&amp;#10;att_sound_ls = []&amp;#10;for row_ind in range(len(att_audio_df)):&amp;#10;    new_att_sound = sound.Sound(&amp;#10;        att_audio_df.iloc[row_ind]['file_path'],&amp;#10;        name=f'att_sound_{row_ind}', &amp;#10;        stereo=True,&amp;#10;        hamming=True,&amp;#10;        volume=1&amp;#10;    )&amp;#10;    att_sound_ls.append(new_att_sound)&amp;#10;&amp;#10;# form list of trial 'area of interest components', 'invisible'&amp;#10;# rectangles that are used for assigning trial visual stimuli&amp;#10;# areas of interest, and checking if participant gaze&amp;#10;# is directed at/'located in' these AOI's&amp;#10;trial_aoi_components = []&amp;#10;for i in range(4):&amp;#10;    new_aoi_comp = visual.Rect(&amp;#10;        win=win,&amp;#10;        name=f'trial_aoi_comp_{i}', &amp;#10;        ori=0, pos=(0, 0), &amp;#10;        size=(TRIAL_AOI_WIDTH, TRIAL_AOI_HEIGHT),&amp;#10;        lineColor=[-1,-1,1], &amp;#10;        colorSpace='rgb', &amp;#10;        opacity=1,&amp;#10;        depth=0.0,&amp;#10;        interpolate=False&amp;#10;    )&amp;#10;    trial_aoi_components.append(new_aoi_comp)&amp;#10;&amp;#10;# form attention grabber 'area of interest component'&amp;#10;att_aoi_comp = visual.Rect(&amp;#10;    win=win,&amp;#10;    name=f'att_aoi_comp', &amp;#10;    ori=0, pos=(0, 0), &amp;#10;    size=(ATT_AOI_WIDTH, ATT_AOI_HEIGHT),&amp;#10;    lineColor=[-1,1,-1], &amp;#10;    colorSpace='rgb', &amp;#10;    opacity=1,&amp;#10;    depth=0.0,&amp;#10;    interpolate=False&amp;#10;)&amp;#10;# since attention grabbers are always presented&amp;#10;# in the centre of the image, and have the same&amp;#10;# maximum width/height, the same AOI component&amp;#10;# can be used for all of them&amp;#10;for img in att_img_ls:&amp;#10;    img.aoi = att_aoi_comp&amp;#10;&amp;#10;# there are issues in PsychoPy related to linear interpolation&amp;#10;# (ie when interpolate=True) for image components, which makes black lines&amp;#10;# appear around some of the images. &amp;#10;# see eg&amp;#10;# https://discourse.psychopy.org/t/borders-on-some-images/9995/6&amp;#10;# for a discussion about this.&amp;#10;# the experiment however needs this interpolation to avoid having really &amp;#10;# jagged image display.&amp;#10;# to get rid of the unwanted 'outlines' mentioned above, this white borders-only&amp;#10;# rectangle component is drawn on top of image components, to hide the outlines.&amp;#10;rect_outlinehider = visual.Rect(&amp;#10;    win=win,&amp;#10;    name='rect_outlinehider',&amp;#10;    units='deg',&amp;#10;    width=2,&amp;#10;    height=2,&amp;#10;    ori=0,&amp;#10;    pos=(0, 0),&amp;#10;    lineWidth=0.8,&amp;#10;    colorSpace='rgb',&amp;#10;    lineColor=[1, 1, 1],&amp;#10;    fillColor=None,&amp;#10;    opacity=None,&amp;#10;    depth=1,&amp;#10;    interpolate=False&amp;#10;)&amp;#10;&amp;#10;# key response component for checking if experimenter hits&amp;#10;# 'a' key to play attention grabbing sound, or 'p' key to&amp;#10;# pause the experiment&amp;#10;key_resp_exp = keyboard.Keyboard()&amp;#10;&amp;#10;# counter for number of trials having passed&amp;#10;trial_counter = 0&amp;#10;&amp;#10;if DEBUG_ON:&amp;#10;    # text component for showing current trial time, for enabling&amp;#10;    # manual/external timing comparisons between eyetracker events&amp;#10;    # and events as seen on camera recording&amp;#10;    text_trialtime = visual.TextStim(win=win, name='text_trialtime',&amp;#10;        text='',&amp;#10;        font='Open Sans',&amp;#10;        units='deg', pos=(0, -6), height=0.8, wrapWidth=None, ori=0.0, &amp;#10;        color='black', colorSpace='rgb', opacity=None, &amp;#10;        languageStyle='LTR',&amp;#10;        depth=0.0&amp;#10;    )&amp;#10;" valType="extendedCode"/>
        <Param name="Begin JS Experiment" updates="constant" val="img_ls = [];&amp;#10;pos_ls = [[0, IMG_CIRCLE_RADIUS], [IMG_CIRCLE_RADIUS, 0], [0, (- IMG_CIRCLE_RADIUS)], [(- IMG_CIRCLE_RADIUS), 0]];&amp;#10;for (var i = 0, _pj_a = 4; (i &lt; _pj_a); i += 1) {&amp;#10;    new visual.ImageStim({&quot;win&quot;: win, &quot;name&quot;: &quot;image&quot;, &quot;image&quot;: null, &quot;mask&quot;: null, &quot;ori&quot;: 0, &quot;pos&quot;: [0, 0.5], &quot;size&quot;: [0.3, 0.3], &quot;color&quot;: [1, 1, 1], &quot;colorSpace&quot;: &quot;rgb&quot;, &quot;opacity&quot;: 1, &quot;flipHoriz&quot;: false, &quot;flipVert&quot;: false, &quot;texRes&quot;: 512, &quot;interpolate&quot;: true, &quot;depth&quot;: 0.0});&amp;#10;}&amp;#10;" valType="extendedCode"/>
        <Param name="Begin JS Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin Routine" updates="constant" val="# calculate start times/delays and durations of stimuli&amp;#10;# attention grabber start time&amp;#10;att_grab_extra_startdelay = random_20thsec((ATT_GRAB_LATEST - ATT_GRAB_EARLIEST)*20)&amp;#10;att_grab_start = ATT_GRAB_EARLIEST + att_grab_extra_startdelay&amp;#10;# auditory stimulus delay after gaze capture&amp;#10;audio_extra_delay = random_20thsec((AUDITORY_MAX_DELAY - AUDITORY_MIN_DELAY)*20)&amp;#10;audio_delay = AUDITORY_MIN_DELAY + audio_extra_delay&amp;#10;# visual stimuli delay after audio onset, and visual stimuli duration&amp;#10;visual_extra_delay = random_20thsec((VISUAL_MAX_DELAY - VISUAL_MIN_DELAY)*20)&amp;#10;visual_delay = VISUAL_MIN_DELAY + visual_extra_delay&amp;#10;visual_extra_dur = random_20thsec((VISUAL_MAX_DUR - VISUAL_MIN_DUR)*20)&amp;#10;visual_dur = VISUAL_MIN_DUR + visual_extra_dur&amp;#10;# duration of 'blank screen', shown after visual stimuli offset&amp;#10;blank_extra_dur = random_20thsec((BLANK_MAX_DUR - BLANK_MIN_DUR)*20)&amp;#10;blank_dur = BLANK_MIN_DUR + blank_extra_dur&amp;#10;&amp;#10;# booleans indicating if the different components are &amp;#10;# active/started (ie shown/played)&amp;#10;att_active = False&amp;#10;audio_started = False&amp;#10;visual_started = False&amp;#10;visual_active = False&amp;#10;&amp;#10;# list that represents the last 30 frames of the experiment, &amp;#10;# that's used for checking if participant's gaze has been directed&amp;#10;# at attention grabber for 200ms out of the last 500ms.&amp;#10;# 200ms corresponds to 12 frames, 500ms corresponds to 30 frames.&amp;#10;# 0's in the list indicate that participant's gaze wasn't directed&amp;#10;# at the attention grabber, while 1's mean that gaze _was_ directed&amp;#10;# at attention grabber for a particular frame. &amp;#10;att_gaze_frame_ls = [0] * 30&amp;#10;# boolean indicating if gaze has been directed at attention grabber &amp;#10;# for 200ms out of the last 500ms&amp;#10;gaze_captured = False&amp;#10;&amp;#10;# timepoints (initialized with very high values,&amp;#10;# to avoid events being preemptively triggered)&amp;#10;# timepoint at which attention grabber starts&amp;#10;# being presented&amp;#10;att_onset_t = 9999999&amp;#10;# timepoint at which gaze is directed at&amp;#10;# attention grabber&amp;#10;gaze_capture_t = 9999999&amp;#10;# timepoint at which sound starts playing&amp;#10;audio_onset_t = 9999999&amp;#10;# timepoint at which visual stimuli start to be presented&amp;#10;visual_onset_t = 9999999&amp;#10;# timepoint at which visual stimuli stop being presented&amp;#10;visual_end_t = 9999999&amp;#10;&amp;#10;# sounds are to be played at the same volume two trials in a row, meaning&amp;#10;# trials come in pairs. accordingly, for all even-numbered trials &amp;#10;# (trial number 0, 2, 4...), a new sound stimulus,&amp;#10;# and a new sound level, is to be used&amp;#10;if trial_counter % 2 == 0:&amp;#10;    # determine which sound should be used, going from start to end of&amp;#10;    # list (and looping around once reaching the end. the looping is what&amp;#10;    # the '% len(...)' part does. if this is confusing, please do a google&amp;#10;    # search for 'python remainder operator'&amp;#10;    sound_index = (trial_counter // 2) % len(sound_ls)&amp;#10;    sound_trial = sound_ls[sound_index]&amp;#10;    # if it's not a silent trial (in which case sound_trial &amp;#10;    # would be a None element), assign a volume pseudorandomly&amp;#10;    if sound_trial is not None:&amp;#10;        assign_pseudorandom_volume(sound_trial)&amp;#10;&amp;#10;# generate a random array of visual stimuli to use&amp;#10;img_ls = get_trial_img_ls()&amp;#10;# assign positions to the visual stimuli, making sure that&amp;#10;# each category's image is in a different position compared to where&amp;#10;# the category's previous image (in the previous trial) appeared&amp;#10;position_indices = generate_position_indices(position_indices)&amp;#10;for image_index, position_index in enumerate(position_indices):&amp;#10;    img_ls[image_index].pos = pos_ls[position_index]&amp;#10;    # assign an 'area of interest' rectangle/component,&amp;#10;    # which is used for checking if gaze is directed at&amp;#10;    # the AOI around the stimulus' position&amp;#10;    img_ls[image_index].aoi = trial_aoi_components[position_index]&amp;#10;    img_ls[image_index].aoi.pos = pos_ls[position_index]&amp;#10;&amp;#10;# randomly pick an attention grabber image&amp;#10;# to use for this trial&amp;#10;att_img = randchoice(att_img_ls, 1)[0]&amp;#10;# extract original size/opacity/y coordinate of attention grabber,&amp;#10;# as they might change (depending on the type&amp;#10;# of attention grabber animation used), meaning these original values&amp;#10;# are necessary to reset the attention grabber component at &amp;#10;# end of routine/trial&amp;#10;att_orig_size = att_img.size&amp;#10;att_orig_opacity = att_img.opacity&amp;#10;att_orig_y_coord = att_img.pos[1]&amp;#10;&amp;#10;# randomly pick which attention grabber&amp;#10;# animation (function) to use&amp;#10;animater_name = randchoice(att_animater_names, 1)[0]&amp;#10;&amp;#10;# reset variables/properties related to keyboard component &amp;#10;# (the one used for checking if&amp;#10;# experimenter wants to play attention grabbing sound, or pause the&amp;#10;# experiment)&amp;#10;key_resp_exp.keys = []&amp;#10;# boolean indicating that the experimenter pressed the 'a'&amp;#10;# key last frame (set to True to prevent any previous routine's key press&amp;#10;# from activating sound at first frame)&amp;#10;soundpress_last_frame = True&amp;#10;# variable for storing time at which last attention grabbing sound&amp;#10;# started playing, to prevent experimenter from playing multiple &amp;#10;# attention grabbing sounds in a row. is initialized to a negative&amp;#10;# value to ensure that no cooldown is applied at start.&amp;#10;att_sound_start_time = -9999&amp;#10;# variable for storing 'active' attention grabbing sound component,&amp;#10;# for being able to stop it before trial start&amp;#10;active_attention_sound = None&amp;#10;# variable for counting the number of times that experimenter plays&amp;#10;# an attention grabbing sound&amp;#10;att_sound_counter = 0&amp;#10;&amp;#10;# dictionary for counting total number of frames for which visual stimuli&amp;#10;# are shown, and how many of these frames are spent looking at each type&amp;#10;# of visual stimulus&amp;#10;gaze_counter_dict = {&amp;#10;    'total': 0,&amp;#10;    'manmade': 0,&amp;#10;    'natural': 0,&amp;#10;    'geometric': 0,&amp;#10;    'social': 0&amp;#10;}&amp;#10;&amp;#10;# if debugging is enabled&amp;#10;if DEBUG_ON:&amp;#10;    # boolean indicating if screenshot has been taken&amp;#10;    trial_screenshot_taken = False&amp;#10;    attgrab_screenshot_taken = False&amp;#10;&amp;#10;# get the time at which the first trial 'frame flip'&amp;#10;# will occur, to save as trial start time&amp;#10;trial_start_time = win.getFutureFlipTime(clock=None)&amp;#10;&amp;#10;# boolean indicating if experiment is currently paused (True) or not (False) &amp;#10;exp_paused = False&amp;#10;# counter that indicates for how long this trial is paused during attention&amp;#10;# grabbing phase. note that this will be a sum, if the trial is repeatedly&amp;#10;# paused&amp;#10;pause_duration = 0&amp;#10;# boolean indicating if pause key was pressed down on last frame&amp;#10;pausepress_last_frame = False" valType="extendedCode"/>
        <Param name="Code Type" updates="None" val="Py" valType="str"/>
        <Param name="Each Frame" updates="constant" val="# time to activate attention grabber?&amp;#10;if t &gt;= att_grab_start and not (att_active or gaze_captured):&amp;#10;    att_active = True&amp;#10;    att_t_start = tThisFlip&amp;#10;&amp;#10;# if experiment is paused, check if experimenter wants to unpause (otherwise&amp;#10;# do nothing)&amp;#10;if att_active and exp_paused:&amp;#10;    this_frame_keys = key_resp_exp.getKeys(&amp;#10;        keyList=['a', 'A', 'p', 'P'],&amp;#10;        waitRelease=False&amp;#10;    )&amp;#10;    # check if experimenter has pressed 'p' or 'P' to pause the experiment&amp;#10;    pausepress_this_frame = any([k.name.lower() == 'p' for k in this_frame_keys])&amp;#10;    new_pausepress = pausepress_this_frame and not pausepress_last_frame&amp;#10;    pausepress_last_frame = pausepress_this_frame&amp;#10;    # end pause&amp;#10;    if new_pausepress:&amp;#10;        pause_duration += trialClock.getTime() - pause_trial_time&amp;#10;        exp_paused = False&amp;#10;&amp;#10;# attention grabber active?&amp;#10;if att_active and not exp_paused:&amp;#10;    # collect any experimenter keyboard input&amp;#10;    this_frame_keys = key_resp_exp.getKeys(&amp;#10;        keyList=['a', 'A', 'p', 'P'],&amp;#10;        waitRelease=False&amp;#10;    )&amp;#10;    # check if experimenter has pressed 'p' or 'P' to pause the experiment&amp;#10;    pausepress_this_frame = any([k.name.lower() == 'p' for k in this_frame_keys])&amp;#10;    new_pausepress = pausepress_this_frame and not pausepress_last_frame&amp;#10;    # update boolean so that it's possible to check if new&amp;#10;    # keyboard press occurs in next frame&amp;#10;    pausepress_last_frame = pausepress_this_frame&amp;#10;    if new_pausepress and not exp_paused:&amp;#10;        # start pause&amp;#10;        pause_trial_time = trialClock.getTime()&amp;#10;        exp_paused = True&amp;#10;#    if exp_paused:&amp;#10;#        continue&amp;#10;    # time passed since attention grabber was activated&amp;#10;    att_passed_t = tThisFlip - att_t_start&amp;#10;    # animate the attention grabber image, based on which&amp;#10;    # animation function was randomly picked in 'Begin Routine'&amp;#10;    if animater_name == 'pulsate':&amp;#10;        pulsate_image(&amp;#10;            att_img,&amp;#10;            att_passed_t,&amp;#10;            att_orig_size&amp;#10;        )&amp;#10;    elif animater_name == 'flicker':&amp;#10;        flicker_image(&amp;#10;            att_img,&amp;#10;            att_passed_t,&amp;#10;            att_orig_opacity&amp;#10;        )&amp;#10;    elif animater_name == 'bounce':&amp;#10;        bounce_image(&amp;#10;            att_img,&amp;#10;            att_passed_t,&amp;#10;            att_orig_y_coord&amp;#10;        )&amp;#10;    # show the attention grabber&amp;#10;    att_img.draw()&amp;#10;    # remove the 'oldest' frame's information about whether participant&amp;#10;    # gaze was then directed at attention grabber&amp;#10;    att_gaze_frame_ls.pop(0)&amp;#10;    # check if gaze (as measured by eye tracker)&amp;#10;    # is within attention grabber's area of interest (AOI)&amp;#10;    gaze_pos = eye_tracker.getPosition()&amp;#10;    if gaze_pos and att_img.aoi.contains(*gaze_pos, units='deg'):&amp;#10;        # save information that participant gazed at attention grabber during&amp;#10;        # this frame&amp;#10;        att_gaze_frame_ls.append(1)&amp;#10;    else:&amp;#10;        # save information that participant did _not_ gaze at&amp;#10;        # attention grabber during this frame&amp;#10;        att_gaze_frame_ls.append(0)&amp;#10;    # check for how many frames, out of the last 30, that participant gaze&amp;#10;    # was directed at attention grabber&amp;#10;    acc_gaze_frames = sum(att_gaze_frame_ls)&amp;#10;    # if participant gaze was directed at attention grabber for&amp;#10;    # at least 12 (200ms) out of the last 30 (500ms) frames, OR if maximum&amp;#10;    # attention grabber duration has passed, signal&amp;#10;    # that attention grabber phase is finished.&amp;#10;    if (acc_gaze_frames &gt;= 12) or (att_passed_t - pause_duration) &gt; ATT_GRAB_MAX_DUR:&amp;#10;        att_active = False&amp;#10;        gaze_captured = True&amp;#10;        gaze_capture_t = tThisFlip&amp;#10;        # send event marker to ioHub/'eyetracker output' that&amp;#10;        # attention grabber phase is ending&amp;#10;        io_connection.sendMessageEvent(f'exp1 trial {trial_counter+1} attention grabber end')&amp;#10;    # check if experimenter has pressed 'a' (also checking for caps 'A') &amp;#10;    # key, meaning they want to play an attention grabbing sound&amp;#10;    soundpress_this_frame = any([k.name.lower() == 'a' for k in this_frame_keys])&amp;#10;    new_soundpress = soundpress_this_frame and not soundpress_last_frame&amp;#10;    # check if enough time has passed since last &amp;#10;    # attention grabbing sound (if any)&amp;#10;    # started playing&amp;#10;    sound_cooldown_passed = (tThisFlip - att_sound_start_time) &gt;= ATT_SOUND_COOLDOWN_TIME&amp;#10;    if new_soundpress and sound_cooldown_passed:&amp;#10;        # make sure that if an attention grabbing sound has been played before,&amp;#10;        # it is completely stopped before the new one is played&amp;#10;        if active_attention_sound:&amp;#10;            active_attention_sound.stop()&amp;#10;        # randomly pick an attention grabbing sound to play&amp;#10;        active_attention_sound = randchoice(att_sound_ls, 1, replace=False)[0]&amp;#10;        active_attention_sound.play()&amp;#10;        att_sound_start_time = tThisFlip&amp;#10;        att_sound_counter += 1&amp;#10;    # update boolean so that it's possible to check if new&amp;#10;    # keyboard press occurs in next frame&amp;#10;    soundpress_last_frame = soundpress_this_frame&amp;#10;&amp;#10;# gaze has been captured and time of delay&amp;#10;# from gaze capture-&gt;sound onset has passed?&amp;#10;if gaze_captured and not audio_started and t &gt;= (gaze_capture_t + audio_delay):&amp;#10;    # if an attention grabbing sound is playing, stop it&amp;#10;    if active_attention_sound:&amp;#10;        active_attention_sound.stop()&amp;#10;    audio_started = True&amp;#10;    # if it's not a silent trial, play the trial's sound component/audio&amp;#10;    # stimulus&amp;#10;    if sound_trial is not None:&amp;#10;        sound_trial.play()&amp;#10;    audio_onset_t = tThisFlip&amp;#10;    # send event marker to ioHub/'eyetracker output' that&amp;#10;    # trial sound is starting (or would have started, if it weren't&amp;#10;    # a silent trial)&amp;#10;    io_connection.sendMessageEvent(f'exp1 trial {trial_counter+1} sound onset')&amp;#10;&amp;#10;# gaze has been captured and time of delay&amp;#10;# from sound onset-&gt;visual stimul onset has passed?&amp;#10;if audio_started and not visual_started and tThisFlip &gt;= (audio_onset_t + visual_delay):&amp;#10;    visual_active = True&amp;#10;    visual_started = True&amp;#10;    visual_onset_t = tThisFlip&amp;#10;    # send event marker to ioHub/'eyetracker output' that&amp;#10;    # trial visual array is shown (on the next frame flip)&amp;#10;    io_connection.sendMessageEvent(f'exp1 trial {trial_counter+1} visual onset')&amp;#10;&amp;#10;# visual stimuli are active?&amp;#10;if visual_active:&amp;#10;    # increment counter for number of frames that visual stimuli are&amp;#10;    # presented for&amp;#10;    gaze_counter_dict['total'] += 1&amp;#10;    for img in img_ls:&amp;#10;        # draw the image itself&amp;#10;        img.draw()&amp;#10;        # copy the image's width/height to the 'outline hider'&amp;#10;        # rectangle component, and draw it to hide&amp;#10;        # unwanted and interpolation-related outlines&amp;#10;        # (this is a hack to circumvent a bug - see the 'begin routine'&amp;#10;        # tab)&amp;#10;        rect_outlinehider.width = img.size[0]&amp;#10;        rect_outlinehider.height = img.size[1]&amp;#10;        rect_outlinehider.pos = img.pos&amp;#10;        rect_outlinehider.draw()&amp;#10;    # check if gaze (as measured by eye tracker)&amp;#10;    # is within any visual stimulus' area of interest (AOI)&amp;#10;    gaze_pos = eye_tracker.getPosition()&amp;#10;    if gaze_pos:&amp;#10;        for img in img_ls:&amp;#10;            if img.aoi.contains(*gaze_pos, units='deg'):&amp;#10;                # increase counter for number of frames&amp;#10;                # spent by participant gazing at image&amp;#10;                # category's images&amp;#10;                gaze_counter_dict[img.img_category] += 1&amp;#10;    # visual stimuli have been shown for&amp;#10;    # set duration?&amp;#10;    if (tThisFlip - visual_onset_t) &gt;= visual_dur:&amp;#10;        visual_active = False&amp;#10;        visual_end_t = tThisFlip&amp;#10;        # send event marker to ioHub/'eyetracker output' that&amp;#10;        # trial visual array is ending (on the next frame flip)&amp;#10;        io_connection.sendMessageEvent(f'exp1 trial {trial_counter+1} visual offset')&amp;#10;&amp;#10;# end blank screen has been shown for its&amp;#10;# intended duration?&amp;#10;if tThisFlip &gt;= (visual_end_t + blank_dur):&amp;#10;    # end trial&amp;#10;    continueRoutine = False&amp;#10;&amp;#10;# if debugging is enabled&amp;#10;if DEBUG_ON:&amp;#10;    # draw areas of interest outlines&amp;#10;    att_aoi_comp.draw()&amp;#10;    for img in img_ls:&amp;#10;        img.aoi.draw()&amp;#10;    # NOTE that taking screenshots of trials, as below, causes very noticeable&amp;#10;    # hiccups in experiment flow/screen refresh time. if this is a problem&amp;#10;    # for timing measurements, comment out the screenshot-related code&amp;#10;    # take screenshot of trial at attention grabber onset&amp;#10;    if not attgrab_screenshot_taken and att_active:&amp;#10;        win.getMovieFrame(buffer='back')&amp;#10;        win.saveMovieFrames(f'exp_snapshots/att_grab_{trial_counter}.png', codec='png')&amp;#10;        attgrab_screenshot_taken = True&amp;#10;    # take screenshot of trial at visual stimuli onset&amp;#10;    if not trial_screenshot_taken and visual_active:&amp;#10;        win.getMovieFrame(buffer='back')&amp;#10;        win.saveMovieFrames(f'exp_snapshots/trial_{trial_counter}.png', codec='png')&amp;#10;        trial_screenshot_taken = True&amp;#10;    # show current experiment/PsychoPy time&amp;#10;    text_trialtime.text = tThisFlipGlobal&amp;#10;    text_trialtime.draw()&amp;#10;" valType="extendedCode"/>
        <Param name="Each JS Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="End Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="End Routine" updates="constant" val="# get trial end time&amp;#10;trial_end_t = t&amp;#10;&amp;#10;# assign attention grabber image component its &amp;#10;# original size/opacity/position, in case they &amp;#10;# were manipulated for animation (depends on what&amp;#10;# type of animation was used)&amp;#10;att_img.size = att_orig_size&amp;#10;att_img.opacity = att_orig_opacity&amp;#10;att_img.pos = (att_img.pos[0], att_orig_y_coord)&amp;#10;&amp;#10;### BEGIN SAVE TRIAL DATA ###&amp;#10;# trial start time (counting from start of experiment)&amp;#10;thisExp.addData('trial_global_start_time', trial_start_time)&amp;#10;&amp;#10;# the time information below is relative to experiment start&amp;#10;&amp;#10;# intended (ie as instructed to PsychoPy - should be in agreement&amp;#10;# with recorded actual times) start times/durations/delays&amp;#10;# (intended) attention grabber start time&amp;#10;thisExp.addData('att_grab_start_time_intended', att_grab_start)&amp;#10;# (intended) auditory stimulus delay after gaze capture&amp;#10;thisExp.addData('gaze_to_audio_delay_intended', audio_delay)&amp;#10;# (intended) visual stimuli delay after audio onset&amp;#10;thisExp.addData('audio_to_visual_delay_intended', visual_delay)&amp;#10;# (intended) visual stimuli duration&amp;#10;thisExp.addData('visual_duration_intended', visual_dur)&amp;#10;# (intended) duration of 'blank screen', shown after visual stimuli offset&amp;#10;thisExp.addData('end_blank_duration_intended', blank_dur)&amp;#10;&amp;#10;# actual (recorded during experiment) time information&amp;#10;# (actual) attention grabber start time&amp;#10;thisExp.addData('att_grab_start_time_actual', att_t_start)&amp;#10;# (actual) gaze captured time (ie time at which participant has&amp;#10;# looked at attention grabber for 200 out of the last 500ms, and&amp;#10;# attention grabber is removed)&amp;#10;thisExp.addData('gaze_captured_time', gaze_capture_t)&amp;#10;# (actual) auditory stimulus onset time&amp;#10;thisExp.addData('audio_onset_time', audio_onset_t)&amp;#10;# (actual) visual stimuli onset time&amp;#10;thisExp.addData('visual_onset_time', visual_onset_t)&amp;#10;# (actual) visual stimuli offset time&amp;#10;thisExp.addData('visual_offset_time', visual_end_t)&amp;#10;# (actual) trial end time&amp;#10;thisExp.addData('trial_end_time', trial_end_t)&amp;#10;&amp;#10;# number of times that experimenter played an attention grabbing&amp;#10;# sound&amp;#10;thisExp.addData('attention_sounds_played', att_sound_counter)&amp;#10;&amp;#10;# total number of frames during which stimuli were shown&amp;#10;# (there are normally 60 frames per second)&amp;#10;thisExp.addData('visual_stimuli_duration_nframes', gaze_counter_dict['total'])&amp;#10;# proportion of visual stimuli presentation time spent by participant&amp;#10;# gazing at the stimuli AOI's, rounded to five decimals&amp;#10;thisExp.addData(&amp;#10;    'visual_social_prop', &amp;#10;    round(gaze_counter_dict['social'] / gaze_counter_dict['total'], 5)&amp;#10;)&amp;#10;thisExp.addData(&amp;#10;    'visual_geometric_prop', &amp;#10;    round(gaze_counter_dict['geometric'] / gaze_counter_dict['total'], 5)&amp;#10;)&amp;#10;thisExp.addData(&amp;#10;    'visual_manmade_prop', &amp;#10;    round(gaze_counter_dict['manmade'] / gaze_counter_dict['total'], 5)&amp;#10;)&amp;#10;thisExp.addData(&amp;#10;    'visual_natural_prop', &amp;#10;    round(gaze_counter_dict['natural'] / gaze_counter_dict['total'], 5)&amp;#10;)&amp;#10;&amp;#10;# visual stimuli characteristics&amp;#10;for img in img_ls:&amp;#10;    # visual stimuli file paths&amp;#10;    thisExp.addData(&amp;#10;        f'visual_{img.img_category}_filepath', img.image&amp;#10;    )&amp;#10;    # visual stimuli position x/y coordinates, in degrees&amp;#10;    thisExp.addData(&amp;#10;        f'visual_{img.img_category}_pos_x', img.pos[0]&amp;#10;    )&amp;#10;    thisExp.addData(&amp;#10;        f'visual_{img.img_category}_pos_y', img.pos[1]&amp;#10;    )&amp;#10;&amp;#10;# audio stimuli characteristics&amp;#10;# if it was a silent trial, save 'silent' as sound filepath,&amp;#10;# and '0' for volume&amp;#10;if sound_trial is None:&amp;#10;    thisExp.addData(&amp;#10;        f'audio_filepath', 'silent',&amp;#10;    )&amp;#10;    thisExp.addData(&amp;#10;        f'audio_volume', 0,&amp;#10;    )&amp;#10;    thisExp.addData(&amp;#10;        f'audio_type', 'silent',&amp;#10;    )&amp;#10;    thisExp.addData(&amp;#10;        f'audio_voice_sex', 'silent',&amp;#10;    )&amp;#10;else:&amp;#10;    thisExp.addData(&amp;#10;        f'audio_filepath', sound_trial.sound,&amp;#10;    )&amp;#10;    thisExp.addData(&amp;#10;        f'audio_volume', sound_trial.volume,&amp;#10;    )&amp;#10;    thisExp.addData(&amp;#10;        f'audio_type', sound_trial.sound_type,&amp;#10;    )&amp;#10;    thisExp.addData(&amp;#10;        f'audio_voice_sex', sound_trial.voice_sex,&amp;#10;    )&amp;#10;&amp;#10;# total (attention grabbing phase) pause duration&amp;#10;thisExp.addData('pause_duration', pause_duration)&amp;#10;&amp;#10;### END SAVE TRIAL DATA ###&amp;#10;&amp;#10;# increment trial counter&amp;#10;trial_counter += 1&amp;#10;" valType="extendedCode"/>
        <Param name="disabled" updates="None" val="False" valType="bool"/>
        <Param name="name" updates="None" val="code_trial" valType="code"/>
      </CodeComponent>
      <CodeComponent name="code_attention_grabber_animations">
        <Param name="Before Experiment" updates="constant" val="# this code snippet defines functions which are&amp;#10;# to be used with attention grabber image components,&amp;#10;# (in separate code snippets' 'each frame' tab)&amp;#10;# to animate them&amp;#10;&amp;#10;def pulsate_image(&amp;#10;    img, &amp;#10;    time_val, &amp;#10;    orig_size,&amp;#10;    rel_amplitude=PULSE_AMP, &amp;#10;    period=PULSE_PERIOD&amp;#10;):&amp;#10;    &quot;&quot;&quot;&amp;#10;    Resizes a passed image component based on a passed&amp;#10;    time (in seconds) and a passed relative amplitude.&amp;#10;    E. g. if the passed relative amplitude is 0.2,&amp;#10;    if this function is called continuously with the&amp;#10;    image component then the image will pulsate&amp;#10;    between 80% and 120% of its original size.&amp;#10;    :param img: ImageStim - image component to be resized/pulsate&amp;#10;    :param time_val: float - current time in seconds (ie the 't'  variable in PsychoPy Builder)&amp;#10;    :param orig_size: 2-element tuple - describes the image's original size, eg (0.3, 0.4)&amp;#10;    :param rel_amplitude: float - relative amplitude, see above&amp;#10;        (defaults to value set in 'setup' routine's code component)&amp;#10;    :param period: float - seconds per complete pulse/cycle&amp;#10;        (defaults to value set in 'setup' routine's code component)&amp;#10;    &quot;&quot;&quot;&amp;#10;    # using the numpy package `sin`, sine, function here. the sine function is ideal for&amp;#10;    # describing cyclical processes. for information about the function and the&amp;#10;    # mathematics involved, see eg https://arrayjson.com/numpy-sin/&amp;#10;    # also, the numpy 'pi' variable, which holds an &amp;#10;    # approximation of the mathematical constant pi, is used&amp;#10;    resize_factor = rel_amplitude * sin(time_val / period * 2 * pi) + 1&amp;#10;    new_size = tuple((resize_factor * x for x in orig_size))&amp;#10;    img.size = new_size&amp;#10;&amp;#10;def flicker_image(&amp;#10;    img, &amp;#10;    time_val, &amp;#10;    orig_opacity=1,&amp;#10;    least_opacity=FLICKER_LEAST_OPACITY, &amp;#10;    period=FLICKER_PERIOD&amp;#10;):&amp;#10;    &quot;&quot;&quot;&amp;#10;    Makes a passed image component 'flicker', ie&amp;#10;    fade in and out, based on a passed&amp;#10;    time (in seconds) and a passed least opacity&amp;#10;    value.&amp;#10;    E. g. if the passed least opacity value is 0.6,&amp;#10;    if this function is called continuously with the&amp;#10;    image component then the image will pulsate&amp;#10;    between 60% and 100% opacity.&amp;#10;    :param img: ImageStim - image component to make fade in/out&amp;#10;    :param time_val: float - current time in seconds (ie the 't'  variable in PsychoPy Builder)&amp;#10;    :param orig_opacity: float - describes the image's original opacity (defaults to 1, ie 100% visibility)&amp;#10;    :param least_opacity: float - least opacity, see above &amp;#10;        (defaults to value set in 'setup' routine's code component)&amp;#10;    :param period: float - seconds per complete fade-in/fade-out cycle &amp;#10;        (defaults to value set in 'setup' routine's code component)&amp;#10;    &quot;&quot;&quot;&amp;#10;    # find 'middle' opacity, eg 0.75 if original opacity is 1 and least_opacity is&amp;#10;    # set to 0.5&amp;#10;    middle_opacity = (orig_opacity + least_opacity)/2&amp;#10;    # find the difference between 'middle' and original opacity, eg 0.25 in the example above&amp;#10;    mid_to_orig_diff = orig_opacity - middle_opacity&amp;#10;    # for info on `sin`, see comment in `pulsate_image` code above&amp;#10;    new_opacity = middle_opacity + mid_to_orig_diff * sin(time_val / period * 2 * pi)&amp;#10;    img.opacity = new_opacity&amp;#10;&amp;#10;def bounce_image(&amp;#10;    img, &amp;#10;    time_val, &amp;#10;    orig_y_coord=0,&amp;#10;    bounce_height=BOUNCE_HEIGHT, &amp;#10;    period=BOUNCE_PERIOD&amp;#10;):&amp;#10;    &quot;&quot;&quot;&amp;#10;    Makes a passed image component 'bounce', ie&amp;#10;    move up and down, based on a passed&amp;#10;    time (in seconds) and a passed bounce height&amp;#10;    value.&amp;#10;    E. g. if the passed bounce height value is 1,&amp;#10;    if this function is called continuously with the&amp;#10;    image component then the image will continuously&amp;#10;    move up 1 degree, go back to original position, go down 1 deg,&amp;#10;    go back to original position, and so on.&amp;#10;    :param img: ImageStim - image component to make bounce&amp;#10;    :param time_val: float - current time in seconds (ie the 't'  variable in PsychoPy Builder)&amp;#10;    :param orig_y_coord: float - describes the image's original y coordinate/position &amp;#10;        (defaults to 0, ie middle of screen)&amp;#10;    :param bounce_height: float - bounce height, see above &amp;#10;        (defaults to value set in 'setup' routine's code component)&amp;#10;    :param period: float - seconds per complete bounce cycle&amp;#10;        (defaults to value set in 'setup' routine's code component)&amp;#10;    &quot;&quot;&quot;&amp;#10;    # calculate new y coordinate&amp;#10;    # for info on `sin`, see comment in `pulsate_image` code above.&amp;#10;    new_y_coord = orig_y_coord + bounce_height * sin(time_val / period * 2 * pi)&amp;#10;    # assign new x-y position. note that this has to be passed as a 2-element&amp;#10;    # tuple, which is why we need to access the x-coordinate of the image component&amp;#10;    # by using `img.pos[0]` here.&amp;#10;    img.pos = (img.pos[0], new_y_coord)&amp;#10;&amp;#10;# form a list of animation function names, which will be used&amp;#10;# to randomly decide, for each trial, which function/animation to use&amp;#10;att_animater_names = ['pulsate', 'flicker', 'bounce']" valType="extendedCode"/>
        <Param name="Before JS Experiment" updates="constant" val="/* Syntax Error: Fix Python code */" valType="extendedCode"/>
        <Param name="Begin Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin JS Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="Code Type" updates="None" val="Py" valType="str"/>
        <Param name="Each Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="Each JS Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="End Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="End Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="disabled" updates="None" val="False" valType="bool"/>
        <Param name="name" updates="None" val="code_attention_grabber_animations" valType="code"/>
      </CodeComponent>
      <CodeComponent name="code_monitor_mirror">
        <Param name="Before Experiment" updates="constant" val="def get_dist_eyetracker(et):&amp;#10;    &quot;&quot;&quot;&amp;#10;    A function for extracting participant('user') distance from &amp;#10;    screen (not really the absolute distance, only the magnitude&amp;#10;    of the vector component that extends 'straight out of' the&amp;#10;    eyetracker, the distance in the 'y' axis according to Tobii). &amp;#10;    Takes in an ioHub 'eyetracker device' object.&amp;#10;    Returns None if neither of the eyes were successfully captured.&amp;#10;    Returns a single numerical value representing mean distance of both eyes,&amp;#10;    or just distance of one if not both could be captured.&amp;#10;    &quot;&quot;&quot;&amp;#10;    # `getLastSample` is not well documented, but includes basically all&amp;#10;    # data about last recording sample from eyetracker in a list.&amp;#10;    # the data that are relevant here happen to be at indices 15 and 34.&amp;#10;    # (at least for a Tobii Spectrum eyetracker - it's possible that&amp;#10;    # corresponding data will be found in other indices for other eyetrackers)&amp;#10;    last_sample = eye_tracker.getLastSample()&amp;#10;    # make extra sure that the last sample is valid, ie is a list of appropriate&amp;#10;    # length&amp;#10;    if not (last_sample and type(last_sample) == list and len(last_sample) &gt; 34):&amp;#10;        return None&amp;#10;    # extract left/right eye Y axis distances from eyetracker&amp;#10;    left_dist = last_sample[15]&amp;#10;    right_dist = last_sample[34]&amp;#10;    left_truthy_not_nan = left_dist and not np.isnan(left_dist)&amp;#10;    right_truthy_not_nan = right_dist and not np.isnan(right_dist)&amp;#10;    if left_truthy_not_nan and right_truthy_not_nan:&amp;#10;        return (left_dist + right_dist) / 2&amp;#10;    if left_truthy_not_nan:&amp;#10;        return left_dist&amp;#10;    if right_truthy_not_nan:&amp;#10;        return right_dist&amp;#10;    return None&amp;#10;" valType="extendedCode"/>
        <Param name="Before JS Experiment" updates="constant" val="/* Syntax Error: Fix Python code */" valType="extendedCode"/>
        <Param name="Begin Experiment" updates="constant" val="# if eyetracker is being mocked (ie a mouse is used&amp;#10;# instead of an actual eyetracker), skip mirroring &amp;#10;# display as described in comments below&amp;#10;if not MOCK_ON:&amp;#10;    # while running the experiment, the experimenter&amp;#10;    # is expected to keep an eye on where the participant&amp;#10;    # is looking and to play attention-grabbing sounds&amp;#10;    # (at beginning of trial, when attention grabber&amp;#10;    # image is displayed) if it seems necessary.&amp;#10;    # therefore, parts of the experiment need to be&amp;#10;    # mirrored to the experimenter's screen. hence,&amp;#10;    # a separate PsychoPy Window instance is created&amp;#10;    # here for the experiment's screen&amp;#10;    win_experimenter = visual.Window(&amp;#10;        size=[1024, 768], &amp;#10;        fullscr=True, &amp;#10;        screen=0, &amp;#10;        winType='pyglet', &amp;#10;        allowGUI=False, &amp;#10;        allowStencil=False,&amp;#10;#        monitor='MONITOR_CENTER_SPEC_NAME', &amp;#10;        color=[1,1,1], &amp;#10;        colorSpace='rgb',&amp;#10;        blendMode='avg', &amp;#10;        useFBO=True, &amp;#10;        units='deg'&amp;#10;    )&amp;#10;&amp;#10;    # a 'dot' for showing to the experimenter where&amp;#10;    # the participant is currently directing their&amp;#10;    # gaze&amp;#10;    gaze_dot = visual.GratingStim(&amp;#10;        win=win_experimenter,&amp;#10;        tex=None,&amp;#10;        mask='gauss',&amp;#10;        pos=(0, 0),&amp;#10;        size=(0.8, 0.8),&amp;#10;        color='green',&amp;#10;        units='deg'&amp;#10;    )&amp;#10;&amp;#10;    # text stimuli for displaying participant&amp;#10;    # distance from tobii eyetracker&amp;#10;    part_dist_info_msg = visual.TextStim(&amp;#10;        win=win_experimenter,&amp;#10;        text=&quot;Participant distance from screen (target range 55-75):&quot;,&amp;#10;        pos=(0, -2),&amp;#10;        height=0.5,&amp;#10;        units='deg',&amp;#10;        autoLog=False,&amp;#10;        color='black'&amp;#10;    )&amp;#10;    part_dist_val_msg = visual.TextStim(&amp;#10;        win=win_experimenter,&amp;#10;        text=&quot;dist_val_msg&quot;,&amp;#10;        pos=(0, -3),&amp;#10;        height=0.5,&amp;#10;        units='deg',&amp;#10;        autoLog=False,&amp;#10;        color='black'&amp;#10;    )&amp;#10;&amp;#10;# message to show if participant distance from eyetracker could not&amp;#10;# be recorded/measured by eyetracker&amp;#10;missed_recording_str = 'Could not capture distance'&amp;#10;&amp;#10;&amp;#10;# the following section enables mirroring all trial stimuli to the experimenter screen&amp;#10;# and is very technical (requires deeper understanding of Python). some more&amp;#10;# info can be found at:&amp;#10;# https://discourse.psychopy.org/t/a-general-approach-to-mirroring-all-stimuli-to-a-secondary-screen/22095&amp;#10;if not MOCK_ON:&amp;#10;    # decorator function for modifying draw&amp;#10;    # methods so that they will draw to mirror (experimenter)&amp;#10;    # window as well&amp;#10;    def make_draw_mirror(draw_fun):&amp;#10;        def mirror_draw_fun(*args, **kwargs):&amp;#10;            draw_fun(win=win_experimenter)&amp;#10;            draw_fun(*args, **kwargs)&amp;#10;        return mirror_draw_fun&amp;#10;&amp;#10;    # decorator function for making&amp;#10;    # a window flip method also trigger flipping of&amp;#10;    # mirror (experimenter) window&amp;#10;    def make_flip_mirror(flip_fun):&amp;#10;        def mirror_flip_fun(*args, **kwargs):&amp;#10;                win_experimenter.flip(*args, **kwargs)&amp;#10;                flip_fun(*args, **kwargs)&amp;#10;        return mirror_flip_fun&amp;#10;&amp;#10;    # form list of all trial stimulus image components&amp;#10;    all_trial_img_ls = (&amp;#10;        att_img_ls +&amp;#10;        img_ls_social +&amp;#10;        img_ls_geo +&amp;#10;        img_ls_manm +&amp;#10;        img_ls_nat&amp;#10;    )&amp;#10;&amp;#10;    for img_stim in all_trial_img_ls:&amp;#10;        # enforce drawing to both 'usual' and mirror&amp;#10;        # screen&amp;#10;        img_stim.draw = make_draw_mirror(img_stim.draw)&amp;#10;" valType="extendedCode"/>
        <Param name="Begin JS Experiment" updates="constant" val="/* Syntax Error: Fix Python code */" valType="extendedCode"/>
        <Param name="Begin JS Routine" updates="constant" val="tr_eyetracker.subscribe_to(tr.EYETRACKER_USER_POSITION_GUIDE, tr_dist_callback, {&quot;as_dictionary&quot;: true});&amp;#10;" valType="extendedCode"/>
        <Param name="Begin Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="Code Type" updates="None" val="Py" valType="str"/>
        <Param name="Each Frame" updates="constant" val="# if eyetracker is being mocked (ie a mouse is used&amp;#10;# instead of an actual eyetracker), skip mirroring &amp;#10;# display as described in comments&amp;#10;if not MOCK_ON:&amp;#10;    # draw areas of interest outlines&amp;#10;    att_aoi_comp.draw(win=win_experimenter)&amp;#10;    for img in img_ls:&amp;#10;        img.aoi.draw(win=win_experimenter)&amp;#10;&amp;#10;    # get latest gaze position coordinates&amp;#10;    gaze_pos_formirror = eye_tracker.getPosition()&amp;#10;&amp;#10;    # check if a valid gaze position could be&amp;#10;    # recorded (in which case the returned value is&amp;#10;    # of type tuple or list, as this is how the&amp;#10;    # coordinates are 'packaged')&amp;#10;    gaze_pos_isvalid = isinstance(&amp;#10;        gaze_pos_formirror,&amp;#10;        (tuple, list)&amp;#10;    )&amp;#10;    &amp;#10;    # update and show message&amp;#10;    # which indicates participant distance from&amp;#10;    # eyetracker&amp;#10;    y_dist = get_dist_eyetracker(eye_tracker)&amp;#10;    if y_dist:&amp;#10;        part_dist_val_msg.setText(y_dist)&amp;#10;    else:&amp;#10;        # if a value is shown currently, replace it with&amp;#10;        # indication that distance could not be measured&amp;#10;        if part_dist_val_msg.text != missed_recording_str:&amp;#10;            part_dist_val_msg.setText(missed_recording_str)&amp;#10;    part_dist_info_msg.draw()&amp;#10;    part_dist_val_msg.draw()&amp;#10;&amp;#10;    # update marker dot to match gaze position&amp;#10;    if gaze_pos_isvalid:&amp;#10;        gaze_dot.pos = gaze_pos_formirror&amp;#10;    gaze_dot.draw()&amp;#10;&amp;#10;    # now that everything has been drawn on the&amp;#10;    # window's 'canvas' , 'flip' the window/screen&amp;#10;    # so that the new information is displayed&amp;#10;    # to the experimenter&amp;#10;    win_experimenter.flip()" valType="extendedCode"/>
        <Param name="Each JS Frame" updates="constant" val="if ((! MOCK_ON)) {&amp;#10;    att_aoi_comp.draw({&quot;win&quot;: win_experimenter});&amp;#10;    for (var img, _pj_c = 0, _pj_a = img_ls, _pj_b = _pj_a.length; (_pj_c &lt; _pj_b); _pj_c += 1) {&amp;#10;        img = _pj_a[_pj_c];&amp;#10;        img.aoi.draw({&quot;win&quot;: win_experimenter});&amp;#10;    }&amp;#10;    gaze_pos_formirror = eye_tracker.getPosition();&amp;#10;    gaze_pos_isvalid = ((gaze_pos_formirror instanceof tuple) || (gaze_pos_formirror instanceof list));&amp;#10;    part_dist_msg.draw();&amp;#10;    if (gaze_pos_isvalid) {&amp;#10;        gaze_dot.pos = gaze_pos_formirror;&amp;#10;    }&amp;#10;    gaze_dot.draw();&amp;#10;    win_experimenter.flip();&amp;#10;}&amp;#10;" valType="extendedCode"/>
        <Param name="End Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Routine" updates="constant" val="tr_eyetracker.unsubscribe_from(tr.EYETRACKER_USER_POSITION_GUIDE, tr_dist_callback);&amp;#10;" valType="extendedCode"/>
        <Param name="End Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="disabled" updates="None" val="False" valType="bool"/>
        <Param name="name" updates="None" val="code_monitor_mirror" valType="code"/>
      </CodeComponent>
    </Routine>
    <Routine name="pause_video">
      <MovieComponent name="movie_pause">
        <Param name="No audio" updates="None" val="False" valType="bool"/>
        <Param name="backend" updates="None" val="moviepy" valType="str"/>
        <Param name="contrast" updates="constant" val="1" valType="num"/>
        <Param name="disabled" updates="None" val="False" valType="bool"/>
        <Param name="durationEstim" updates="None" val="" valType="code"/>
        <Param name="forceEndRoutine" updates="constant" val="True" valType="bool"/>
        <Param name="loop" updates="None" val="False" valType="bool"/>
        <Param name="movie" updates="set every repeat" val="$pvideo_file_path" valType="file"/>
        <Param name="name" updates="None" val="movie_pause" valType="code"/>
        <Param name="opacity" updates="constant" val="" valType="num"/>
        <Param name="ori" updates="constant" val="0" valType="num"/>
        <Param name="pos" updates="constant" val="(0, 0)" valType="list"/>
        <Param name="saveStartStop" updates="None" val="True" valType="bool"/>
        <Param name="size" updates="constant" val="(PVIDEO_WIDTH_DEG, PVIDEO_HEIGHT_DEG)" valType="list"/>
        <Param name="startEstim" updates="None" val="" valType="code"/>
        <Param name="startType" updates="None" val="time (s)" valType="str"/>
        <Param name="startVal" updates="None" val="0.0" valType="code"/>
        <Param name="stopType" updates="None" val="duration (s)" valType="str"/>
        <Param name="stopVal" updates="constant" val="8" valType="code"/>
        <Param name="syncScreenRefresh" updates="None" val="True" valType="bool"/>
        <Param name="units" updates="None" val="deg" valType="str"/>
      </MovieComponent>
      <CodeComponent name="code_eyetracker_message_pause">
        <Param name="Before Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Before JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin JS Routine" updates="constant" val="io_connection.sendMessageEvent(`trial ${trial_counter} start`);&amp;#10;" valType="extendedCode"/>
        <Param name="Begin Routine" updates="constant" val="# send 'pause video start' message to ioHub (ie the eye tracker), &amp;#10;# that will be included in the next eye tracker &amp;#10;# data record&amp;#10;io_connection.sendMessageEvent(f'exp1 pause video after trial {trial_counter} start')&amp;#10;" valType="extendedCode"/>
        <Param name="Code Type" updates="None" val="Py" valType="str"/>
        <Param name="Each Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="Each JS Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="End Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Routine" updates="constant" val="/* Syntax Error: Fix Python code */" valType="extendedCode"/>
        <Param name="End Routine" updates="constant" val="# send 'pause video end' message to ioHub/the eye tracker &amp;#10;# that will be included in the next eye tracker &amp;#10;# data record&amp;#10;io_connection.sendMessageEvent(f'exp1 pause video after trial {trial_counter} end')&amp;#10;" valType="extendedCode"/>
        <Param name="disabled" updates="None" val="False" valType="bool"/>
        <Param name="name" updates="None" val="code_eyetracker_message_pause" valType="code"/>
      </CodeComponent>
      <CodeComponent name="code_monitor_mirror_pause">
        <Param name="Before Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Before JS Experiment" updates="constant" val="/* Syntax Error: Fix Python code */" valType="extendedCode"/>
        <Param name="Begin Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin JS Experiment" updates="constant" val="/* Syntax Error: Fix Python code */" valType="extendedCode"/>
        <Param name="Begin JS Routine" updates="constant" val="tr_eyetracker.subscribe_to(tr.EYETRACKER_USER_POSITION_GUIDE, tr_dist_callback, {&quot;as_dictionary&quot;: true});&amp;#10;" valType="extendedCode"/>
        <Param name="Begin Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="Code Type" updates="None" val="Py" valType="str"/>
        <Param name="Each Frame" updates="constant" val="# if eyetracker is being mocked (ie a mouse is used&amp;#10;# instead of an actual eyetracker), skip mirroring &amp;#10;# display as described in comments&amp;#10;if not MOCK_ON:&amp;#10;    # get latest gaze position coordinates&amp;#10;    gaze_pos_formirror = eye_tracker.getPosition()&amp;#10;&amp;#10;    # check if a valid gaze position could be&amp;#10;    # recorded (in which case the returned value is&amp;#10;    # of type tuple or list, as this is how the&amp;#10;    # coordinates are 'packaged')&amp;#10;    gaze_pos_isvalid = isinstance(&amp;#10;        gaze_pos_formirror,&amp;#10;        (tuple, list)&amp;#10;    )&amp;#10;    &amp;#10;    # update and show message&amp;#10;    # which indicates participant distance from&amp;#10;    # eyetracker&amp;#10;    y_dist = get_dist_eyetracker(eye_tracker)&amp;#10;    if y_dist:&amp;#10;        part_dist_val_msg.setText(y_dist)&amp;#10;    else:&amp;#10;        # if a value is shown currently, replace it with&amp;#10;        # indication that distance could not be measured&amp;#10;        if part_dist_val_msg.text != missed_recording_str:&amp;#10;            part_dist_val_msg.setText(missed_recording_str)&amp;#10;    part_dist_info_msg.draw()&amp;#10;    part_dist_val_msg.draw()&amp;#10;&amp;#10;    # update marker dot to match gaze position&amp;#10;    if gaze_pos_isvalid:&amp;#10;        gaze_dot.pos = gaze_pos_formirror&amp;#10;    gaze_dot.draw()&amp;#10;&amp;#10;    # now that everything has been drawn on the&amp;#10;    # window's 'canvas' , 'flip' the window/screen&amp;#10;    # so that the new information is displayed&amp;#10;    # to the experimenter&amp;#10;    win_experimenter.flip()" valType="extendedCode"/>
        <Param name="Each JS Frame" updates="constant" val="if ((! MOCK_ON)) {&amp;#10;    att_aoi_comp.draw({&quot;win&quot;: win_experimenter});&amp;#10;    for (var img, _pj_c = 0, _pj_a = img_ls, _pj_b = _pj_a.length; (_pj_c &lt; _pj_b); _pj_c += 1) {&amp;#10;        img = _pj_a[_pj_c];&amp;#10;        img.aoi.draw({&quot;win&quot;: win_experimenter});&amp;#10;    }&amp;#10;    gaze_pos_formirror = eye_tracker.getPosition();&amp;#10;    gaze_pos_isvalid = ((gaze_pos_formirror instanceof tuple) || (gaze_pos_formirror instanceof list));&amp;#10;    part_dist_msg.draw();&amp;#10;    if (gaze_pos_isvalid) {&amp;#10;        gaze_dot.pos = gaze_pos_formirror;&amp;#10;    }&amp;#10;    gaze_dot.draw();&amp;#10;    win_experimenter.flip();&amp;#10;}&amp;#10;" valType="extendedCode"/>
        <Param name="End Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Routine" updates="constant" val="tr_eyetracker.unsubscribe_from(tr.EYETRACKER_USER_POSITION_GUIDE, tr_dist_callback);&amp;#10;" valType="extendedCode"/>
        <Param name="End Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="disabled" updates="None" val="False" valType="bool"/>
        <Param name="name" updates="None" val="code_monitor_mirror_pause" valType="code"/>
      </CodeComponent>
      <CodeComponent name="code_pvideo">
        <Param name="Before Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Before JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin JS Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="Begin Routine" updates="constant" val="" valType="extendedCode"/>
        <Param name="Code Type" updates="None" val="Auto-&gt;JS" valType="str"/>
        <Param name="Each Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="Each JS Frame" updates="constant" val="" valType="extendedCode"/>
        <Param name="End Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Experiment" updates="constant" val="" valType="extendedCode"/>
        <Param name="End JS Routine" updates="constant" val="psychoJS.experiment.addData(&quot;pause_video_fpath&quot;, pvideo_file_path);&amp;#10;" valType="extendedCode"/>
        <Param name="End Routine" updates="constant" val="thisExp.addData('pause_video_fpath', pvideo_file_path)" valType="extendedCode"/>
        <Param name="disabled" updates="None" val="False" valType="bool"/>
        <Param name="name" updates="None" val="code_pvideo" valType="code"/>
      </CodeComponent>
    </Routine>
  </Routines>
  <Flow>
    <Routine name="setup"/>
    <Routine name="start_screen"/>
    <LoopInitiator loopType="TrialHandler" name="trials_pause_block_loop">
      <Param name="Selected rows" updates="None" val="" valType="str"/>
      <Param name="conditions" updates="None" val="[OrderedDict([('pvideo_file_path', 'stimuli/pause_videos/scenes_1.mp4'), ('pvideo_width', 1920), ('pvideo_height', 1080)]), OrderedDict([('pvideo_file_path', 'stimuli/pause_videos/scenes_1s.mp4'), ('pvideo_width', 1920), ('pvideo_height', 1080)]), OrderedDict([('pvideo_file_path', 'stimuli/pause_videos/scenes_3.mp4'), ('pvideo_width', 1920), ('pvideo_height', 1080)]), OrderedDict([('pvideo_file_path', 'stimuli/pause_videos/scenes_3s.mp4'), ('pvideo_width', 1920), ('pvideo_height', 1080)]), OrderedDict([('pvideo_file_path', 'stimuli/pause_videos/scenes_4.mp4'), ('pvideo_width', 1920), ('pvideo_height', 1080)]), OrderedDict([('pvideo_file_path', 'stimuli/pause_videos/scenes_4s.mp4'), ('pvideo_width', 1920), ('pvideo_height', 1080)]), OrderedDict([('pvideo_file_path', 'stimuli/pause_videos/scenes_5.mp4'), ('pvideo_width', 1920), ('pvideo_height', 1080)]), OrderedDict([('pvideo_file_path', 'stimuli/pause_videos/scenes_5s.mp4'), ('pvideo_width', 1920), ('pvideo_height', 1080)])]" valType="str"/>
      <Param name="conditionsFile" updates="None" val="stimuli_specifications\pause_videos_specifications.csv" valType="file"/>
      <Param name="endPoints" updates="None" val="[0, 1]" valType="num"/>
      <Param name="isTrials" updates="None" val="True" valType="bool"/>
      <Param name="loopType" updates="None" val="sequential" valType="str"/>
      <Param name="nReps" updates="None" val="1" valType="num"/>
      <Param name="name" updates="None" val="trials_pause_block_loop" valType="code"/>
      <Param name="random seed" updates="None" val="" valType="code"/>
    </LoopInitiator>
    <LoopInitiator loopType="TrialHandler" name="trial_loop">
      <Param name="Selected rows" updates="None" val="" valType="str"/>
      <Param name="conditions" updates="None" val="None" valType="str"/>
      <Param name="conditionsFile" updates="None" val="" valType="file"/>
      <Param name="endPoints" updates="None" val="[0, 1]" valType="num"/>
      <Param name="isTrials" updates="None" val="True" valType="bool"/>
      <Param name="loopType" updates="None" val="random" valType="str"/>
      <Param name="nReps" updates="None" val="8" valType="num"/>
      <Param name="name" updates="None" val="trial_loop" valType="code"/>
      <Param name="random seed" updates="None" val="" valType="code"/>
    </LoopInitiator>
    <Routine name="trial"/>
    <LoopTerminator name="trial_loop"/>
    <Routine name="pause_video"/>
    <LoopTerminator name="trials_pause_block_loop"/>
    <Routine name="end_screen"/>
  </Flow>
</PsychoPy2experiment>
